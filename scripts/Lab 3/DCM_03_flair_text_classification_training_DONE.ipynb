{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DCM_03_flair_text_classification_training_DONE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CsMM5WnpA_i0",
        "Y5c9Fz6-BZq6"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8UEeL963baI",
        "outputId": "d3f24766-d5e7-472f-dece-fe391615a64d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb0el2_68Zhq"
      },
      "source": [
        "# PREPROCESSING\n",
        "\n",
        "Functions to load and cleanup the data for training with Flair.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvjWbWcwZf1W"
      },
      "source": [
        "import csv\n",
        "import re\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "# load data\n",
        "def load_data(fnames):\n",
        "    data = []\n",
        "    for fname in fnames:\n",
        "        data.append(pd.read_csv(fname, sep='\\t', encoding='utf-8'))\n",
        "    data = pd.concat(data)\n",
        "    targets = set(data['Target'])\n",
        "    return data, list(targets)\n",
        "\n",
        "# pre-process tweets\n",
        "def cleanup(tweet):\n",
        "    \"\"\"we remove urls, hashtags and user symbols\"\"\"\n",
        "    tweet = re.sub(r\"http\\S+\", \"\", tweet.replace(\"#\", \"\").replace(\"@\", \"\").replace('\\n', ' ').replace('\\t', ' '))\n",
        "    return tweet\n",
        " \n",
        "#to do 1.1\n",
        "def format_label_to_flair(label):\n",
        "  label= \"__label__\" + label\n",
        "  return label\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECydcLTg9xRV"
      },
      "source": [
        "# Loading and Pre-processing the datasets\n",
        "\n",
        "\n",
        "We load and cleanup the data using the functions defined above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlGilGPV-Ipl"
      },
      "source": [
        "## ASSIGNMENT 1\n",
        "\n",
        "+ TODO: Write the format_label_to_flair(label) function above to obtain the label column in the format shown below.\n",
        "+ TODO: Write the required code to load, cleanup and format the labels for both the training and test sets obtaining the output shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "z-SKnx5F8hjP",
        "outputId": "f3f04fc9-09c9-4218-c67a-2839f2b9a261"
      },
      "source": [
        "# data path. trial data used as training too.\n",
        "trial_file = \"/content/drive/MyDrive/2022-ILTAPP/datasets/stance-semeval2016/semeval2016-task6-trialdata.utf-8.txt\"\n",
        "train_file = \"/content/drive/MyDrive/2022-ILTAPP/datasets/stance-semeval2016/semeval2016-task6-trainingdata.utf-8.txt\"\n",
        "test_file = \"/content/drive/MyDrive/2022-ILTAPP/datasets/stance-semeval2016/SemEval2016-Task6-subtaskA-testdata-gold.txt\"\n",
        "\n",
        "# TODO write your code here\n",
        "\n",
        "# load train / test \n",
        "training_data, targets = load_data([train_file, trial_file])\n",
        "test_data, _ = load_data([test_file])\n",
        "\n",
        "#Column Clean_tweet\n",
        "\n",
        "training_data[\"Clean_tweet\"] = \" \"\n",
        "test_data[\"Clean_tweet\"]= \" \"\n",
        "\n",
        "#Training\n",
        "\n",
        "for i in range(len(training_data)):\n",
        "  training_data[\"Clean_tweet\"].iloc[i] = cleanup(training_data[\"Tweet\"].iloc[i])\n",
        "  training_data[\"Stance\"].iloc[i] = format_label_to_flair(training_data[\"Stance\"].iloc[i])\n",
        "\n",
        "#Test\n",
        "for i in range(len(test_data)):\n",
        "  test_data[\"Clean_tweet\"].iloc[i] = cleanup(test_data[\"Tweet\"].iloc[i])\n",
        "  test_data[\"Stance\"].iloc[i] = format_label_to_flair(test_data[\"Stance\"].iloc[i])\n",
        "\n",
        "display(training_data)\n",
        "display(test_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_block(indexer, value, name)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-235430ef-4a46-46b0-ad15-22281e58d686\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Target</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Stance</th>\n",
              "      <th>Clean_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>101</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>dear lord thank u for all of ur blessings forg...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>dear lord thank u for all of ur blessings forg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>102</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>Blessed are the peacemakers, for they shall be...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>Blessed are the peacemakers, for they shall be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>103</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>I am not conformed to this world. I am transfo...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>I am not conformed to this world. I am transfo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>104</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>Salah should be prayed with #focus and #unders...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>Salah should be prayed with focus and understa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>105</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>And stay in your houses and do not display you...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>And stay in your houses and do not display you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>96</td>\n",
              "      <td>Legalization of Abortion</td>\n",
              "      <td>@Corey_Frizzell @PEILiberalParty and most Isla...</td>\n",
              "      <td>__label__NONE</td>\n",
              "      <td>Corey_Frizzell PEILiberalParty and most Island...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>97</td>\n",
              "      <td>Legalization of Abortion</td>\n",
              "      <td>@Docjp Pressure? It's their job and they are f...</td>\n",
              "      <td>__label__NONE</td>\n",
              "      <td>Docjp Pressure? It's their job and they are fa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>98</td>\n",
              "      <td>Legalization of Abortion</td>\n",
              "      <td>I love how #liberals only accuse #conservative...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>I love how liberals only accuse conservatives ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>99</td>\n",
              "      <td>Legalization of Abortion</td>\n",
              "      <td>Help your friend figure out how they're going ...</td>\n",
              "      <td>__label__NONE</td>\n",
              "      <td>Help your friend figure out how they're going ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>100</td>\n",
              "      <td>Legalization of Abortion</td>\n",
              "      <td>@JimDDaniels1 Plenty of reasons for Christians...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>JimDDaniels1 Plenty of reasons for Christians ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2914 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-235430ef-4a46-46b0-ad15-22281e58d686')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-235430ef-4a46-46b0-ad15-22281e58d686 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-235430ef-4a46-46b0-ad15-22281e58d686');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     ID  ...                                        Clean_tweet\n",
              "0   101  ...  dear lord thank u for all of ur blessings forg...\n",
              "1   102  ...  Blessed are the peacemakers, for they shall be...\n",
              "2   103  ...  I am not conformed to this world. I am transfo...\n",
              "3   104  ...  Salah should be prayed with focus and understa...\n",
              "4   105  ...  And stay in your houses and do not display you...\n",
              "..  ...  ...                                                ...\n",
              "95   96  ...  Corey_Frizzell PEILiberalParty and most Island...\n",
              "96   97  ...  Docjp Pressure? It's their job and they are fa...\n",
              "97   98  ...  I love how liberals only accuse conservatives ...\n",
              "98   99  ...  Help your friend figure out how they're going ...\n",
              "99  100  ...  JimDDaniels1 Plenty of reasons for Christians ...\n",
              "\n",
              "[2914 rows x 5 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b508d7a4-0f64-44bc-be20-c72d892cee1a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Target</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Stance</th>\n",
              "      <th>Clean_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10001</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>He who exalts himself shall      be humbled; a...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>He who exalts himself shall      be humbled; a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10002</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>RT @prayerbullets: I remove Nehushtan -previou...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>RT prayerbullets: I remove Nehushtan -previous...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10003</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>@Brainman365 @heidtjj @BenjaminLives I have so...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>Brainman365 heidtjj BenjaminLives I have sough...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10004</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>#God is utterly powerless without Human interv...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>God is utterly powerless without Human interve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10005</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>@David_Cameron   Miracles of #Multiculturalism...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>David_Cameron   Miracles of Multiculturalism  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>11245</td>\n",
              "      <td>Legalization of Abortion</td>\n",
              "      <td>@MetalheadMonty @tom_six I followed him before...</td>\n",
              "      <td>__label__NONE</td>\n",
              "      <td>MetalheadMonty tom_six I followed him before I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1245</th>\n",
              "      <td>11246</td>\n",
              "      <td>Legalization of Abortion</td>\n",
              "      <td>For he who avenges blood remembers, he does no...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>For he who avenges blood remembers, he does no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1246</th>\n",
              "      <td>11247</td>\n",
              "      <td>Legalization of Abortion</td>\n",
              "      <td>Life is sacred on all levels. Abortion does no...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>Life is sacred on all levels. Abortion does no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1247</th>\n",
              "      <td>11248</td>\n",
              "      <td>Legalization of Abortion</td>\n",
              "      <td>@ravensymone U refer to \"WE\" which =\"YOU\" &amp; a ...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>ravensymone U refer to \"WE\" which =\"YOU\" &amp; a m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1248</th>\n",
              "      <td>11249</td>\n",
              "      <td>Legalization of Abortion</td>\n",
              "      <td>Al Robertson's mom #DuckDynasty  chose life as...</td>\n",
              "      <td>__label__AGAINST</td>\n",
              "      <td>Al Robertson's mom DuckDynasty  chose life as ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1249 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b508d7a4-0f64-44bc-be20-c72d892cee1a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b508d7a4-0f64-44bc-be20-c72d892cee1a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b508d7a4-0f64-44bc-be20-c72d892cee1a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         ID  ...                                        Clean_tweet\n",
              "0     10001  ...  He who exalts himself shall      be humbled; a...\n",
              "1     10002  ...  RT prayerbullets: I remove Nehushtan -previous...\n",
              "2     10003  ...  Brainman365 heidtjj BenjaminLives I have sough...\n",
              "3     10004  ...  God is utterly powerless without Human interve...\n",
              "4     10005  ...  David_Cameron   Miracles of Multiculturalism  ...\n",
              "...     ...  ...                                                ...\n",
              "1244  11245  ...  MetalheadMonty tom_six I followed him before I...\n",
              "1245  11246  ...  For he who avenges blood remembers, he does no...\n",
              "1246  11247  ...  Life is sacred on all levels. Abortion does no...\n",
              "1247  11248  ...  ravensymone U refer to \"WE\" which =\"YOU\" & a m...\n",
              "1248  11249  ...  Al Robertson's mom DuckDynasty  chose life as ...\n",
              "\n",
              "[1249 rows x 5 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIT3p9vL-BLv"
      },
      "source": [
        "## ASSIGNMENT 2\n",
        "\n",
        "From the training_data and testing_data dataframes obtained in the previous step, perform the following steps:\n",
        "\n",
        "+ TODO: Iterate over the targets to write a training and test set in the format required by Flair to train a text classifier:\n",
        "\n",
        "```\n",
        "__label__AGAINST first_tweet_in_dataframe\n",
        "__label__NONE second_tweet_in_dataframe\n",
        "__label__FAVOR third_tweet_in_dataframe\n",
        "```\n",
        "*NOTE: One tweet per line*\n",
        "\n",
        "The result should be a training set and test set in files for each target. For example:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "train.Feminist Movement.txt\n",
        "test.Feminist Movement.txt\n",
        "```\n",
        "\n",
        "HINT: You just need to use the Stance and Clean_tweet columns from the training_data and test_data dataframes and write them to a csv with \"\\t\" as separator.\n",
        "\n",
        "https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md#reading-a-text-classification-dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc-5nX2DESYy"
      },
      "source": [
        "# TODO Write your code here\n",
        "\n",
        "\n",
        "data_folder = \"/content/drive/MyDrive/2022-ILTAPP/datasets/stance-semeval2016\"\n",
        "\n",
        "# Target = Feminist Movement...\n",
        "for target in targets:\n",
        "\n",
        "  # Collect only rows of this target\n",
        "  train = training_data[training_data['Target'] == target]\n",
        "  test = test_data[test_data['Target'] == target]\n",
        "\n",
        "  # TRAIN\n",
        "  # Open file (train.Feminist Movement.txt)\n",
        "  with open(f'{data_folder}train.{target}.txt', 'w') as f:\n",
        "    \n",
        "    # Obtain the columns we are interested in in list format\n",
        "    stance = train['Stance'].tolist()\n",
        "    cleanTweet = train['Clean_tweet'].tolist()\n",
        "    \n",
        "    # Get each item in the list\n",
        "    for st, ct in zip(stance, cleanTweet):\n",
        "\n",
        "      # Write in the txt file the stance and the tweet in each line \n",
        "      f.write(f\"{st}\\t{ct}\\n\")\n",
        "\n",
        "  # TEST\n",
        "  with open(f'{data_folder}test.{target}.txt', 'w') as f:\n",
        "    stance = test['Stance'].tolist()\n",
        "    cleanTweet = test['Clean_tweet'].tolist()\n",
        "    for st, ct in zip(stance, cleanTweet):\n",
        "      f.write(f\"{st}\\t{ct}\\n\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsMM5WnpA_i0"
      },
      "source": [
        "# INSTALL FLAIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8okVUTM6W513",
        "outputId": "98c32757-5823-4368-82ca-9668c577c9a0"
      },
      "source": [
        "!pip install flair==0.8"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flair==0.8\n",
            "  Downloading flair-0.8-py3-none-any.whl (277 kB)\n",
            "\u001b[K     |████████████████████████████████| 277 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 45.0 MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair==0.8) (2019.12.20)\n",
            "Collecting transformers>=4.0.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair==0.8) (0.8.9)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.8) (3.2.2)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair==0.8) (0.1.2)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.8) (4.62.3)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 44.7 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 35.8 MB/s \n",
            "\u001b[?25hCollecting numpy<1.20.0\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 36.6 MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair==0.8) (4.2.6)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.8) (3.6.0)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.8) (1.10.0+cu111)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.8) (1.0.2)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair==0.8) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.8) (3.6.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.8) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.8) (1.15.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair==0.8) (1.13.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair==0.8) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair==0.8) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair==0.8) (2.6.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair==0.8) (4.0.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair==0.8) (0.16.0)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.8) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.8) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.8) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.8) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.8) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.8) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.8) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.8) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.8) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair==0.8) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair==0.8) (3.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair==0.8) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 26.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 36.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->flair==0.8) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.8) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair==0.8) (7.1.2)\n",
            "Building wheels for collected packages: gdown, mpld3, overrides, sqlitedict, langdetect\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9692 sha256=d0063403537efcdff84db731491382c8c73eda66af6f11a66fd2fc23b3fe658f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=374d1741cb2475f066d01df65019567634a732f534324ac2c8335cb6162559b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=584503c3d45563743b448da034fadc93bf6698f24b5f1a2c5e1b9a1869baabca\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14393 sha256=f1075edc23006032c44e878b49684cddd41a7ff555cd40eb75cfa9f7888c331e\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=927ebcfd7f107d7f2575a4c0eace2edbf6a9ccb851dbf59a3c6ad32a3f711999\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "Successfully built gdown mpld3 overrides sqlitedict langdetect\n",
            "Installing collected packages: numpy, requests, pyyaml, importlib-metadata, tokenizers, sentencepiece, sacremoses, overrides, huggingface-hub, transformers, sqlitedict, segtok, mpld3, langdetect, konoha, janome, gdown, ftfy, deprecated, bpemb, flair\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.1\n",
            "    Uninstalling importlib-metadata-4.11.1:\n",
            "      Successfully uninstalled importlib-metadata-4.11.1\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 deprecated-1.2.13 flair-0.8 ftfy-6.1.1 gdown-3.12.2 huggingface-hub-0.4.0 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 numpy-1.19.5 overrides-3.1.0 pyyaml-6.0 requests-2.27.1 sacremoses-0.0.47 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gve0M5UeBFKb"
      },
      "source": [
        "# Load the training data created in ASSIGNMENT 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmZ9QqxJYkzi",
        "outputId": "b59b998b-aa3e-4fcd-a0a2-26e73a9cf70f"
      },
      "source": [
        "from flair.data import Corpus                                                                                                                                                           \n",
        "from flair.datasets import ClassificationCorpus\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, FastTextEmbeddings, DocumentRNNEmbeddings                                                             \n",
        "from flair.models import TextClassifier                                                                                                                                                 \n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "# get the corpus\n",
        "corpus_folder = '/content/drive/MyDrive/2022-ILTAPP/datasets/'\n",
        "corpus:Corpus = ClassificationCorpus(corpus_folder,\n",
        "                                      train_file='stance-semeval2016train.Feminist Movement.txt',\n",
        "                                      test_file='stance-semeval2016test.Feminist Movement.txt')\n",
        "\n",
        "# create the label dictionary\n",
        "label_dict = corpus.make_label_dictionary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:50:50,334 Reading data from /content/drive/MyDrive/2022-ILTAPP/datasets\n",
            "2022-03-03 15:50:50,338 Train: /content/drive/MyDrive/2022-ILTAPP/datasets/stance-semeval2016train.Feminist Movement.txt\n",
            "2022-03-03 15:50:50,340 Dev: None\n",
            "2022-03-03 15:50:50,348 Test: /content/drive/MyDrive/2022-ILTAPP/datasets/stance-semeval2016test.Feminist Movement.txt\n",
            "2022-03-03 15:50:50,404 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 883/883 [00:02<00:00, 417.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:50:53,130 [b'FAVOR', b'AGAINST', b'NONE']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr1lylJjBRNk"
      },
      "source": [
        "# INSTALL Word Embeddings and instantiate the TextClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec3MkXzBgxVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00fd6a03-3c51-4c73-d2c4-aa7f833fa53c"
      },
      "source": [
        "# 3. make a list of word embeddings\n",
        "word_embeddings = [WordEmbeddings('en-crawl')]\n",
        "# 4. initialize document embedding by passing list of word embeddings                                                                                                                                                                                                    \n",
        "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
        "                                                                   hidden_size=512,\n",
        "                                                                   reproject_words=True,\n",
        "                                                                   reproject_words_dimension=256,\n",
        "                                                                   rnn_type='LSTM')                                                                                                                                                                           \n",
        "# 5. create the text classifier\n",
        "classifier = TextClassifier(document_embeddings, \n",
        "                            label_dictionary=label_dict)\n",
        "# 6. initialize the text classifier trainer\n",
        "trainer = ModelTrainer(classifier, corpus)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:50:54,618 https://flair.informatik.hu-berlin.de/resources/embeddings/token/en-fasttext-crawl-300d-1M.vectors.npy not found in cache, downloading to /tmp/tmpmdh8feez\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200000128/1200000128 [02:04<00:00, 9606528.24B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:53:00,387 copying /tmp/tmpmdh8feez to cache at /root/.flair/embeddings/en-fasttext-crawl-300d-1M.vectors.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:53:04,531 removing temp file /tmp/tmpmdh8feez\n",
            "2022-03-03 15:53:05,799 https://flair.informatik.hu-berlin.de/resources/embeddings/token/en-fasttext-crawl-300d-1M not found in cache, downloading to /tmp/tmp19q2xra2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 39323680/39323680 [00:05<00:00, 6865521.75B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:53:12,385 copying /tmp/tmp19q2xra2 to cache at /root/.flair/embeddings/en-fasttext-crawl-300d-1M\n",
            "2022-03-03 15:53:12,441 removing temp file /tmp/tmp19q2xra2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5c9Fz6-BZq6"
      },
      "source": [
        "# Run the trainer for feminism target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZK1jIkVbl8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf6a35c-3505-4610-e2b7-aac5b11ff93c"
      },
      "source": [
        "# 7. start the training\n",
        "trainer.train('flair-feminism-model',\n",
        "              train_with_dev=False,\n",
        "              max_epochs=50)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:53:26,976 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:53:26,985 Model: \"TextClassifier(\n",
            "  (document_embeddings): DocumentRNNEmbeddings(\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): WordEmbeddings('en-crawl')\n",
            "    )\n",
            "    (word_reprojection_map): Linear(in_features=300, out_features=256, bias=True)\n",
            "    (rnn): LSTM(256, 512, batch_first=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2022-03-03 15:53:26,989 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:53:26,991 Corpus: \"Corpus: 598 train + 66 dev + 285 test sentences\"\n",
            "2022-03-03 15:53:26,993 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:53:26,997 Parameters:\n",
            "2022-03-03 15:53:26,999  - learning_rate: \"0.1\"\n",
            "2022-03-03 15:53:27,002  - mini_batch_size: \"32\"\n",
            "2022-03-03 15:53:27,004  - patience: \"3\"\n",
            "2022-03-03 15:53:27,006  - anneal_factor: \"0.5\"\n",
            "2022-03-03 15:53:27,008  - max_epochs: \"50\"\n",
            "2022-03-03 15:53:27,010  - shuffle: \"True\"\n",
            "2022-03-03 15:53:27,012  - train_with_dev: \"False\"\n",
            "2022-03-03 15:53:27,014  - batch_growth_annealing: \"False\"\n",
            "2022-03-03 15:53:27,016 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:53:27,020 Model training base path: \"flair-feminism-model\"\n",
            "2022-03-03 15:53:27,023 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:53:27,024 Device: cuda:0\n",
            "2022-03-03 15:53:27,026 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:53:27,070 Embeddings storage mode: cpu\n",
            "2022-03-03 15:53:27,074 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:53:28,104 epoch 1 - iter 1/19 - loss 1.11736906 - samples/sec: 111.58 - lr: 0.100000\n",
            "2022-03-03 15:53:28,256 epoch 1 - iter 2/19 - loss 1.09507221 - samples/sec: 340.62 - lr: 0.100000\n",
            "2022-03-03 15:53:28,360 epoch 1 - iter 3/19 - loss 1.09333181 - samples/sec: 316.91 - lr: 0.100000\n",
            "2022-03-03 15:53:28,467 epoch 1 - iter 4/19 - loss 1.09546259 - samples/sec: 314.57 - lr: 0.100000\n",
            "2022-03-03 15:53:28,563 epoch 1 - iter 5/19 - loss 1.09712903 - samples/sec: 353.56 - lr: 0.100000\n",
            "2022-03-03 15:53:28,663 epoch 1 - iter 6/19 - loss 1.09067760 - samples/sec: 328.37 - lr: 0.100000\n",
            "2022-03-03 15:53:28,782 epoch 1 - iter 7/19 - loss 1.08118110 - samples/sec: 305.87 - lr: 0.100000\n",
            "2022-03-03 15:53:28,874 epoch 1 - iter 8/19 - loss 1.08217441 - samples/sec: 379.14 - lr: 0.100000\n",
            "2022-03-03 15:53:28,977 epoch 1 - iter 9/19 - loss 1.09181692 - samples/sec: 318.78 - lr: 0.100000\n",
            "2022-03-03 15:53:29,063 epoch 1 - iter 10/19 - loss 1.09485635 - samples/sec: 383.95 - lr: 0.100000\n",
            "2022-03-03 15:53:29,147 epoch 1 - iter 11/19 - loss 1.09330658 - samples/sec: 390.61 - lr: 0.100000\n",
            "2022-03-03 15:53:29,247 epoch 1 - iter 12/19 - loss 1.09277364 - samples/sec: 324.65 - lr: 0.100000\n",
            "2022-03-03 15:53:29,334 epoch 1 - iter 13/19 - loss 1.09366398 - samples/sec: 375.13 - lr: 0.100000\n",
            "2022-03-03 15:53:29,419 epoch 1 - iter 14/19 - loss 1.09257332 - samples/sec: 407.95 - lr: 0.100000\n",
            "2022-03-03 15:53:29,507 epoch 1 - iter 15/19 - loss 1.07985281 - samples/sec: 408.35 - lr: 0.100000\n",
            "2022-03-03 15:53:29,599 epoch 1 - iter 16/19 - loss 1.05894791 - samples/sec: 389.16 - lr: 0.100000\n",
            "2022-03-03 15:53:29,691 epoch 1 - iter 17/19 - loss 1.03534185 - samples/sec: 388.36 - lr: 0.100000\n",
            "2022-03-03 15:53:29,779 epoch 1 - iter 18/19 - loss 1.02481045 - samples/sec: 413.03 - lr: 0.100000\n",
            "2022-03-03 15:53:29,844 epoch 1 - iter 19/19 - loss 0.99771810 - samples/sec: 555.16 - lr: 0.100000\n",
            "2022-03-03 15:53:30,009 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:53:30,011 EPOCH 1 done: loss 0.9977 - lr 0.1000000\n",
            "2022-03-03 15:53:32,527 DEV : loss 1.441864013671875 - score 0.3485\n",
            "2022-03-03 15:53:32,598 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:53:48,828 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:53:49,869 epoch 2 - iter 1/19 - loss 0.88349587 - samples/sec: 232.32 - lr: 0.100000\n",
            "2022-03-03 15:53:49,956 epoch 2 - iter 2/19 - loss 0.88273591 - samples/sec: 404.54 - lr: 0.100000\n",
            "2022-03-03 15:53:50,041 epoch 2 - iter 3/19 - loss 0.92115589 - samples/sec: 502.46 - lr: 0.100000\n",
            "2022-03-03 15:53:50,123 epoch 2 - iter 4/19 - loss 0.99228820 - samples/sec: 479.62 - lr: 0.100000\n",
            "2022-03-03 15:53:50,200 epoch 2 - iter 5/19 - loss 0.98933924 - samples/sec: 454.48 - lr: 0.100000\n",
            "2022-03-03 15:53:50,274 epoch 2 - iter 6/19 - loss 0.99086664 - samples/sec: 530.93 - lr: 0.100000\n",
            "2022-03-03 15:53:50,352 epoch 2 - iter 7/19 - loss 1.01932321 - samples/sec: 460.66 - lr: 0.100000\n",
            "2022-03-03 15:53:50,419 epoch 2 - iter 8/19 - loss 1.00349057 - samples/sec: 494.88 - lr: 0.100000\n",
            "2022-03-03 15:53:50,488 epoch 2 - iter 9/19 - loss 0.99643257 - samples/sec: 567.76 - lr: 0.100000\n",
            "2022-03-03 15:53:50,576 epoch 2 - iter 10/19 - loss 0.99323159 - samples/sec: 587.57 - lr: 0.100000\n",
            "2022-03-03 15:53:50,638 epoch 2 - iter 11/19 - loss 0.99865353 - samples/sec: 532.41 - lr: 0.100000\n",
            "2022-03-03 15:53:50,706 epoch 2 - iter 12/19 - loss 1.01082369 - samples/sec: 514.39 - lr: 0.100000\n",
            "2022-03-03 15:53:50,771 epoch 2 - iter 13/19 - loss 1.00621081 - samples/sec: 503.99 - lr: 0.100000\n",
            "2022-03-03 15:53:50,841 epoch 2 - iter 14/19 - loss 1.01164492 - samples/sec: 470.10 - lr: 0.100000\n",
            "2022-03-03 15:53:50,917 epoch 2 - iter 15/19 - loss 1.01432793 - samples/sec: 529.69 - lr: 0.100000\n",
            "2022-03-03 15:53:50,983 epoch 2 - iter 16/19 - loss 1.01289153 - samples/sec: 499.04 - lr: 0.100000\n",
            "2022-03-03 15:53:51,051 epoch 2 - iter 17/19 - loss 1.01711815 - samples/sec: 484.99 - lr: 0.100000\n",
            "2022-03-03 15:53:51,125 epoch 2 - iter 18/19 - loss 1.01580430 - samples/sec: 533.32 - lr: 0.100000\n",
            "2022-03-03 15:53:51,179 epoch 2 - iter 19/19 - loss 1.01129076 - samples/sec: 631.63 - lr: 0.100000\n",
            "2022-03-03 15:53:51,615 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:53:51,621 EPOCH 2 done: loss 1.0113 - lr 0.1000000\n",
            "2022-03-03 15:53:52,898 DEV : loss 1.3048807382583618 - score 0.3485\n",
            "2022-03-03 15:53:52,974 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:54:08,065 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:54:09,186 epoch 3 - iter 1/19 - loss 1.06938171 - samples/sec: 182.00 - lr: 0.100000\n",
            "2022-03-03 15:54:09,280 epoch 3 - iter 2/19 - loss 1.04791045 - samples/sec: 391.74 - lr: 0.100000\n",
            "2022-03-03 15:54:09,359 epoch 3 - iter 3/19 - loss 0.98856113 - samples/sec: 479.40 - lr: 0.100000\n",
            "2022-03-03 15:54:09,462 epoch 3 - iter 4/19 - loss 1.03177269 - samples/sec: 398.57 - lr: 0.100000\n",
            "2022-03-03 15:54:09,545 epoch 3 - iter 5/19 - loss 1.01172208 - samples/sec: 406.68 - lr: 0.100000\n",
            "2022-03-03 15:54:09,643 epoch 3 - iter 6/19 - loss 1.01008655 - samples/sec: 393.06 - lr: 0.100000\n",
            "2022-03-03 15:54:09,752 epoch 3 - iter 7/19 - loss 1.00106055 - samples/sec: 390.51 - lr: 0.100000\n",
            "2022-03-03 15:54:09,826 epoch 3 - iter 8/19 - loss 1.00533150 - samples/sec: 447.02 - lr: 0.100000\n",
            "2022-03-03 15:54:09,911 epoch 3 - iter 9/19 - loss 1.00592519 - samples/sec: 402.63 - lr: 0.100000\n",
            "2022-03-03 15:54:10,003 epoch 3 - iter 10/19 - loss 1.00498722 - samples/sec: 356.61 - lr: 0.100000\n",
            "2022-03-03 15:54:10,077 epoch 3 - iter 11/19 - loss 1.00598226 - samples/sec: 445.36 - lr: 0.100000\n",
            "2022-03-03 15:54:10,162 epoch 3 - iter 12/19 - loss 1.00036756 - samples/sec: 471.70 - lr: 0.100000\n",
            "2022-03-03 15:54:10,244 epoch 3 - iter 13/19 - loss 0.99601975 - samples/sec: 477.55 - lr: 0.100000\n",
            "2022-03-03 15:54:10,322 epoch 3 - iter 14/19 - loss 0.99254488 - samples/sec: 450.48 - lr: 0.100000\n",
            "2022-03-03 15:54:10,400 epoch 3 - iter 15/19 - loss 0.98962272 - samples/sec: 502.32 - lr: 0.100000\n",
            "2022-03-03 15:54:10,487 epoch 3 - iter 16/19 - loss 0.99314476 - samples/sec: 374.87 - lr: 0.100000\n",
            "2022-03-03 15:54:10,571 epoch 3 - iter 17/19 - loss 0.99569755 - samples/sec: 416.16 - lr: 0.100000\n",
            "2022-03-03 15:54:10,651 epoch 3 - iter 18/19 - loss 0.99395819 - samples/sec: 407.82 - lr: 0.100000\n",
            "2022-03-03 15:54:10,719 epoch 3 - iter 19/19 - loss 0.99745490 - samples/sec: 482.63 - lr: 0.100000\n",
            "2022-03-03 15:54:11,172 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:54:11,178 EPOCH 3 done: loss 0.9975 - lr 0.1000000\n",
            "2022-03-03 15:54:12,557 DEV : loss 1.3046139478683472 - score 0.3485\n",
            "2022-03-03 15:54:12,643 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:54:28,194 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:54:29,239 epoch 4 - iter 1/19 - loss 1.10720241 - samples/sec: 175.79 - lr: 0.100000\n",
            "2022-03-03 15:54:29,326 epoch 4 - iter 2/19 - loss 1.07206637 - samples/sec: 388.09 - lr: 0.100000\n",
            "2022-03-03 15:54:29,416 epoch 4 - iter 3/19 - loss 1.05620293 - samples/sec: 484.53 - lr: 0.100000\n",
            "2022-03-03 15:54:29,499 epoch 4 - iter 4/19 - loss 1.01889975 - samples/sec: 428.63 - lr: 0.100000\n",
            "2022-03-03 15:54:29,589 epoch 4 - iter 5/19 - loss 1.02922577 - samples/sec: 388.78 - lr: 0.100000\n",
            "2022-03-03 15:54:29,691 epoch 4 - iter 6/19 - loss 1.03629606 - samples/sec: 473.30 - lr: 0.100000\n",
            "2022-03-03 15:54:29,775 epoch 4 - iter 7/19 - loss 1.04585715 - samples/sec: 391.78 - lr: 0.100000\n",
            "2022-03-03 15:54:29,855 epoch 4 - iter 8/19 - loss 1.03889749 - samples/sec: 438.87 - lr: 0.100000\n",
            "2022-03-03 15:54:29,937 epoch 4 - iter 9/19 - loss 1.01844867 - samples/sec: 408.17 - lr: 0.100000\n",
            "2022-03-03 15:54:30,023 epoch 4 - iter 10/19 - loss 1.01142673 - samples/sec: 378.91 - lr: 0.100000\n",
            "2022-03-03 15:54:30,102 epoch 4 - iter 11/19 - loss 1.00629862 - samples/sec: 448.20 - lr: 0.100000\n",
            "2022-03-03 15:54:30,195 epoch 4 - iter 12/19 - loss 1.00225999 - samples/sec: 498.73 - lr: 0.100000\n",
            "2022-03-03 15:54:30,273 epoch 4 - iter 13/19 - loss 0.99522736 - samples/sec: 420.90 - lr: 0.100000\n",
            "2022-03-03 15:54:30,348 epoch 4 - iter 14/19 - loss 0.99473836 - samples/sec: 472.78 - lr: 0.100000\n",
            "2022-03-03 15:54:30,436 epoch 4 - iter 15/19 - loss 0.99285942 - samples/sec: 372.31 - lr: 0.100000\n",
            "2022-03-03 15:54:30,513 epoch 4 - iter 16/19 - loss 0.98458470 - samples/sec: 430.05 - lr: 0.100000\n",
            "2022-03-03 15:54:30,594 epoch 4 - iter 17/19 - loss 0.98496924 - samples/sec: 403.67 - lr: 0.100000\n",
            "2022-03-03 15:54:30,681 epoch 4 - iter 18/19 - loss 0.98646724 - samples/sec: 434.79 - lr: 0.100000\n",
            "2022-03-03 15:54:30,741 epoch 4 - iter 19/19 - loss 0.99187230 - samples/sec: 636.46 - lr: 0.100000\n",
            "2022-03-03 15:54:31,204 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:54:31,206 EPOCH 4 done: loss 0.9919 - lr 0.1000000\n",
            "2022-03-03 15:54:32,581 DEV : loss 1.307172179222107 - score 0.3485\n",
            "2022-03-03 15:54:32,664 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 15:54:32,672 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:54:33,602 epoch 5 - iter 1/19 - loss 1.02531409 - samples/sec: 171.17 - lr: 0.100000\n",
            "2022-03-03 15:54:33,699 epoch 5 - iter 2/19 - loss 1.04429615 - samples/sec: 388.55 - lr: 0.100000\n",
            "2022-03-03 15:54:33,788 epoch 5 - iter 3/19 - loss 1.00386175 - samples/sec: 445.26 - lr: 0.100000\n",
            "2022-03-03 15:54:33,878 epoch 5 - iter 4/19 - loss 0.98253644 - samples/sec: 393.95 - lr: 0.100000\n",
            "2022-03-03 15:54:33,968 epoch 5 - iter 5/19 - loss 0.97521230 - samples/sec: 457.15 - lr: 0.100000\n",
            "2022-03-03 15:54:34,083 epoch 5 - iter 6/19 - loss 0.98471392 - samples/sec: 409.88 - lr: 0.100000\n",
            "2022-03-03 15:54:34,172 epoch 5 - iter 7/19 - loss 0.97273072 - samples/sec: 410.79 - lr: 0.100000\n",
            "2022-03-03 15:54:34,251 epoch 5 - iter 8/19 - loss 0.98415866 - samples/sec: 413.28 - lr: 0.100000\n",
            "2022-03-03 15:54:34,329 epoch 5 - iter 9/19 - loss 0.97752768 - samples/sec: 420.62 - lr: 0.100000\n",
            "2022-03-03 15:54:34,418 epoch 5 - iter 10/19 - loss 0.96468734 - samples/sec: 367.53 - lr: 0.100000\n",
            "2022-03-03 15:54:34,502 epoch 5 - iter 11/19 - loss 0.95394306 - samples/sec: 395.45 - lr: 0.100000\n",
            "2022-03-03 15:54:34,575 epoch 5 - iter 12/19 - loss 0.96499338 - samples/sec: 517.54 - lr: 0.100000\n",
            "2022-03-03 15:54:34,654 epoch 5 - iter 13/19 - loss 0.95695233 - samples/sec: 415.21 - lr: 0.100000\n",
            "2022-03-03 15:54:34,728 epoch 5 - iter 14/19 - loss 0.96700232 - samples/sec: 446.24 - lr: 0.100000\n",
            "2022-03-03 15:54:34,810 epoch 5 - iter 15/19 - loss 0.96965994 - samples/sec: 398.77 - lr: 0.100000\n",
            "2022-03-03 15:54:34,886 epoch 5 - iter 16/19 - loss 0.97162506 - samples/sec: 434.36 - lr: 0.100000\n",
            "2022-03-03 15:54:34,961 epoch 5 - iter 17/19 - loss 0.98259868 - samples/sec: 520.36 - lr: 0.100000\n",
            "2022-03-03 15:54:35,039 epoch 5 - iter 18/19 - loss 0.98375155 - samples/sec: 456.53 - lr: 0.100000\n",
            "2022-03-03 15:54:35,100 epoch 5 - iter 19/19 - loss 0.98593683 - samples/sec: 622.81 - lr: 0.100000\n",
            "2022-03-03 15:54:35,602 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:54:35,604 EPOCH 5 done: loss 0.9859 - lr 0.1000000\n",
            "2022-03-03 15:54:36,942 DEV : loss 1.2645920515060425 - score 0.3485\n",
            "2022-03-03 15:54:37,013 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:54:51,741 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:54:52,792 epoch 6 - iter 1/19 - loss 1.03254950 - samples/sec: 140.12 - lr: 0.100000\n",
            "2022-03-03 15:54:52,921 epoch 6 - iter 2/19 - loss 1.06133252 - samples/sec: 312.11 - lr: 0.100000\n",
            "2022-03-03 15:54:53,008 epoch 6 - iter 3/19 - loss 1.06804327 - samples/sec: 388.38 - lr: 0.100000\n",
            "2022-03-03 15:54:53,133 epoch 6 - iter 4/19 - loss 1.06570417 - samples/sec: 394.78 - lr: 0.100000\n",
            "2022-03-03 15:54:53,214 epoch 6 - iter 5/19 - loss 1.03491650 - samples/sec: 404.89 - lr: 0.100000\n",
            "2022-03-03 15:54:53,301 epoch 6 - iter 6/19 - loss 1.01720979 - samples/sec: 397.31 - lr: 0.100000\n",
            "2022-03-03 15:54:53,386 epoch 6 - iter 7/19 - loss 1.00730793 - samples/sec: 428.25 - lr: 0.100000\n",
            "2022-03-03 15:54:53,467 epoch 6 - iter 8/19 - loss 0.99738701 - samples/sec: 408.57 - lr: 0.100000\n",
            "2022-03-03 15:54:53,548 epoch 6 - iter 9/19 - loss 0.99771674 - samples/sec: 436.99 - lr: 0.100000\n",
            "2022-03-03 15:54:53,632 epoch 6 - iter 10/19 - loss 0.99793660 - samples/sec: 437.11 - lr: 0.100000\n",
            "2022-03-03 15:54:53,720 epoch 6 - iter 11/19 - loss 1.00047444 - samples/sec: 374.91 - lr: 0.100000\n",
            "2022-03-03 15:54:53,793 epoch 6 - iter 12/19 - loss 0.98680402 - samples/sec: 447.18 - lr: 0.100000\n",
            "2022-03-03 15:54:53,873 epoch 6 - iter 13/19 - loss 0.98161911 - samples/sec: 411.06 - lr: 0.100000\n",
            "2022-03-03 15:54:53,967 epoch 6 - iter 14/19 - loss 0.98592604 - samples/sec: 348.11 - lr: 0.100000\n",
            "2022-03-03 15:54:54,046 epoch 6 - iter 15/19 - loss 0.97918432 - samples/sec: 415.27 - lr: 0.100000\n",
            "2022-03-03 15:54:54,123 epoch 6 - iter 16/19 - loss 0.98281231 - samples/sec: 490.64 - lr: 0.100000\n",
            "2022-03-03 15:54:54,202 epoch 6 - iter 17/19 - loss 0.97748343 - samples/sec: 456.71 - lr: 0.100000\n",
            "2022-03-03 15:54:54,286 epoch 6 - iter 18/19 - loss 0.98340388 - samples/sec: 433.27 - lr: 0.100000\n",
            "2022-03-03 15:54:54,352 epoch 6 - iter 19/19 - loss 0.97881655 - samples/sec: 535.12 - lr: 0.100000\n",
            "2022-03-03 15:54:54,831 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:54:54,832 EPOCH 6 done: loss 0.9788 - lr 0.1000000\n",
            "2022-03-03 15:54:56,228 DEV : loss 1.3059132099151611 - score 0.3485\n",
            "2022-03-03 15:54:56,305 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 15:54:56,307 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:54:57,248 epoch 7 - iter 1/19 - loss 1.00998008 - samples/sec: 164.50 - lr: 0.100000\n",
            "2022-03-03 15:54:57,390 epoch 7 - iter 2/19 - loss 0.98958686 - samples/sec: 358.38 - lr: 0.100000\n",
            "2022-03-03 15:54:57,476 epoch 7 - iter 3/19 - loss 0.96159240 - samples/sec: 392.09 - lr: 0.100000\n",
            "2022-03-03 15:54:57,564 epoch 7 - iter 4/19 - loss 0.95632358 - samples/sec: 395.52 - lr: 0.100000\n",
            "2022-03-03 15:54:57,659 epoch 7 - iter 5/19 - loss 0.96104265 - samples/sec: 344.71 - lr: 0.100000\n",
            "2022-03-03 15:54:57,738 epoch 7 - iter 6/19 - loss 0.97289218 - samples/sec: 440.50 - lr: 0.100000\n",
            "2022-03-03 15:54:57,820 epoch 7 - iter 7/19 - loss 0.96843243 - samples/sec: 416.46 - lr: 0.100000\n",
            "2022-03-03 15:54:57,898 epoch 7 - iter 8/19 - loss 0.97374792 - samples/sec: 505.86 - lr: 0.100000\n",
            "2022-03-03 15:54:57,979 epoch 7 - iter 9/19 - loss 0.98939008 - samples/sec: 433.03 - lr: 0.100000\n",
            "2022-03-03 15:54:58,059 epoch 7 - iter 10/19 - loss 0.98378893 - samples/sec: 446.39 - lr: 0.100000\n",
            "2022-03-03 15:54:58,140 epoch 7 - iter 11/19 - loss 0.97897396 - samples/sec: 401.71 - lr: 0.100000\n",
            "2022-03-03 15:54:58,220 epoch 7 - iter 12/19 - loss 0.97932768 - samples/sec: 427.53 - lr: 0.100000\n",
            "2022-03-03 15:54:58,290 epoch 7 - iter 13/19 - loss 0.97377800 - samples/sec: 466.74 - lr: 0.100000\n",
            "2022-03-03 15:54:58,362 epoch 7 - iter 14/19 - loss 0.96878723 - samples/sec: 488.72 - lr: 0.100000\n",
            "2022-03-03 15:54:58,441 epoch 7 - iter 15/19 - loss 0.97727487 - samples/sec: 449.32 - lr: 0.100000\n",
            "2022-03-03 15:54:58,538 epoch 7 - iter 16/19 - loss 0.97738574 - samples/sec: 419.11 - lr: 0.100000\n",
            "2022-03-03 15:54:58,610 epoch 7 - iter 17/19 - loss 0.97424367 - samples/sec: 452.86 - lr: 0.100000\n",
            "2022-03-03 15:54:58,695 epoch 7 - iter 18/19 - loss 0.97391295 - samples/sec: 386.46 - lr: 0.100000\n",
            "2022-03-03 15:54:58,748 epoch 7 - iter 19/19 - loss 0.97294814 - samples/sec: 700.28 - lr: 0.100000\n",
            "2022-03-03 15:54:59,221 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:54:59,222 EPOCH 7 done: loss 0.9729 - lr 0.1000000\n",
            "2022-03-03 15:55:00,594 DEV : loss 1.2765865325927734 - score 0.3485\n",
            "2022-03-03 15:55:00,678 BAD EPOCHS (no improvement): 2\n",
            "2022-03-03 15:55:00,680 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:55:01,644 epoch 8 - iter 1/19 - loss 0.83952439 - samples/sec: 194.83 - lr: 0.100000\n",
            "2022-03-03 15:55:01,738 epoch 8 - iter 2/19 - loss 0.94332308 - samples/sec: 388.16 - lr: 0.100000\n",
            "2022-03-03 15:55:01,828 epoch 8 - iter 3/19 - loss 0.97177692 - samples/sec: 480.97 - lr: 0.100000\n",
            "2022-03-03 15:55:01,922 epoch 8 - iter 4/19 - loss 0.98080418 - samples/sec: 422.76 - lr: 0.100000\n",
            "2022-03-03 15:55:02,008 epoch 8 - iter 5/19 - loss 0.96504323 - samples/sec: 422.79 - lr: 0.100000\n",
            "2022-03-03 15:55:02,100 epoch 8 - iter 6/19 - loss 0.96413834 - samples/sec: 373.83 - lr: 0.100000\n",
            "2022-03-03 15:55:02,189 epoch 8 - iter 7/19 - loss 0.97045836 - samples/sec: 413.70 - lr: 0.100000\n",
            "2022-03-03 15:55:02,267 epoch 8 - iter 8/19 - loss 0.97688752 - samples/sec: 419.67 - lr: 0.100000\n",
            "2022-03-03 15:55:03,929 epoch 8 - iter 9/19 - loss 0.98866139 - samples/sec: 618.30 - lr: 0.100000\n",
            "2022-03-03 15:55:04,001 epoch 8 - iter 10/19 - loss 0.98622393 - samples/sec: 557.87 - lr: 0.100000\n",
            "2022-03-03 15:55:04,066 epoch 8 - iter 11/19 - loss 0.97373373 - samples/sec: 507.85 - lr: 0.100000\n",
            "2022-03-03 15:55:04,131 epoch 8 - iter 12/19 - loss 0.96704617 - samples/sec: 510.35 - lr: 0.100000\n",
            "2022-03-03 15:55:04,200 epoch 8 - iter 13/19 - loss 0.96890910 - samples/sec: 510.21 - lr: 0.100000\n",
            "2022-03-03 15:55:04,268 epoch 8 - iter 14/19 - loss 0.96815433 - samples/sec: 481.28 - lr: 0.100000\n",
            "2022-03-03 15:55:04,338 epoch 8 - iter 15/19 - loss 0.96004369 - samples/sec: 516.20 - lr: 0.100000\n",
            "2022-03-03 15:55:04,422 epoch 8 - iter 16/19 - loss 0.95946868 - samples/sec: 443.15 - lr: 0.100000\n",
            "2022-03-03 15:55:04,487 epoch 8 - iter 17/19 - loss 0.95826499 - samples/sec: 510.26 - lr: 0.100000\n",
            "2022-03-03 15:55:04,559 epoch 8 - iter 18/19 - loss 0.96051678 - samples/sec: 523.82 - lr: 0.100000\n",
            "2022-03-03 15:55:04,613 epoch 8 - iter 19/19 - loss 0.96950726 - samples/sec: 621.82 - lr: 0.100000\n",
            "2022-03-03 15:55:05,091 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:55:05,093 EPOCH 8 done: loss 0.9695 - lr 0.1000000\n",
            "2022-03-03 15:55:06,468 DEV : loss 1.2544654607772827 - score 0.3636\n",
            "2022-03-03 15:55:06,552 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:55:21,065 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:55:22,169 epoch 9 - iter 1/19 - loss 1.01146030 - samples/sec: 312.48 - lr: 0.100000\n",
            "2022-03-03 15:55:22,246 epoch 9 - iter 2/19 - loss 0.95550966 - samples/sec: 476.74 - lr: 0.100000\n",
            "2022-03-03 15:55:22,319 epoch 9 - iter 3/19 - loss 0.92097171 - samples/sec: 461.49 - lr: 0.100000\n",
            "2022-03-03 15:55:22,401 epoch 9 - iter 4/19 - loss 0.93872415 - samples/sec: 398.43 - lr: 0.100000\n",
            "2022-03-03 15:55:22,479 epoch 9 - iter 5/19 - loss 0.96287252 - samples/sec: 461.41 - lr: 0.100000\n",
            "2022-03-03 15:55:22,555 epoch 9 - iter 6/19 - loss 0.97436544 - samples/sec: 496.64 - lr: 0.100000\n",
            "2022-03-03 15:55:22,637 epoch 9 - iter 7/19 - loss 0.96186881 - samples/sec: 490.93 - lr: 0.100000\n",
            "2022-03-03 15:55:22,704 epoch 9 - iter 8/19 - loss 0.95176077 - samples/sec: 521.74 - lr: 0.100000\n",
            "2022-03-03 15:55:22,782 epoch 9 - iter 9/19 - loss 0.94653523 - samples/sec: 513.50 - lr: 0.100000\n",
            "2022-03-03 15:55:22,858 epoch 9 - iter 10/19 - loss 0.94172576 - samples/sec: 443.84 - lr: 0.100000\n",
            "2022-03-03 15:55:22,940 epoch 9 - iter 11/19 - loss 0.93222794 - samples/sec: 459.35 - lr: 0.100000\n",
            "2022-03-03 15:55:23,009 epoch 9 - iter 12/19 - loss 0.92898345 - samples/sec: 555.68 - lr: 0.100000\n",
            "2022-03-03 15:55:23,087 epoch 9 - iter 13/19 - loss 0.93246687 - samples/sec: 539.63 - lr: 0.100000\n",
            "2022-03-03 15:55:23,151 epoch 9 - iter 14/19 - loss 0.94340952 - samples/sec: 515.79 - lr: 0.100000\n",
            "2022-03-03 15:55:23,227 epoch 9 - iter 15/19 - loss 0.95106976 - samples/sec: 541.25 - lr: 0.100000\n",
            "2022-03-03 15:55:23,297 epoch 9 - iter 16/19 - loss 0.95213365 - samples/sec: 478.83 - lr: 0.100000\n",
            "2022-03-03 15:55:23,369 epoch 9 - iter 17/19 - loss 0.94985417 - samples/sec: 453.53 - lr: 0.100000\n",
            "2022-03-03 15:55:23,440 epoch 9 - iter 18/19 - loss 0.94795872 - samples/sec: 523.94 - lr: 0.100000\n",
            "2022-03-03 15:55:23,496 epoch 9 - iter 19/19 - loss 0.96222427 - samples/sec: 637.18 - lr: 0.100000\n",
            "2022-03-03 15:55:23,919 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:55:23,920 EPOCH 9 done: loss 0.9622 - lr 0.1000000\n",
            "2022-03-03 15:55:25,277 DEV : loss 1.2129359245300293 - score 0.3788\n",
            "2022-03-03 15:55:25,352 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:55:39,939 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:55:41,027 epoch 10 - iter 1/19 - loss 0.94770110 - samples/sec: 245.86 - lr: 0.100000\n",
            "2022-03-03 15:55:41,109 epoch 10 - iter 2/19 - loss 0.98109937 - samples/sec: 474.61 - lr: 0.100000\n",
            "2022-03-03 15:55:41,186 epoch 10 - iter 3/19 - loss 0.98691195 - samples/sec: 490.66 - lr: 0.100000\n",
            "2022-03-03 15:55:41,272 epoch 10 - iter 4/19 - loss 0.99797268 - samples/sec: 477.56 - lr: 0.100000\n",
            "2022-03-03 15:55:41,346 epoch 10 - iter 5/19 - loss 0.98262578 - samples/sec: 450.00 - lr: 0.100000\n",
            "2022-03-03 15:55:41,420 epoch 10 - iter 6/19 - loss 0.98536224 - samples/sec: 481.32 - lr: 0.100000\n",
            "2022-03-03 15:55:41,509 epoch 10 - iter 7/19 - loss 0.97589774 - samples/sec: 484.80 - lr: 0.100000\n",
            "2022-03-03 15:55:41,571 epoch 10 - iter 8/19 - loss 0.96910898 - samples/sec: 539.99 - lr: 0.100000\n",
            "2022-03-03 15:55:41,661 epoch 10 - iter 9/19 - loss 0.96337395 - samples/sec: 566.87 - lr: 0.100000\n",
            "2022-03-03 15:55:41,736 epoch 10 - iter 10/19 - loss 0.94805974 - samples/sec: 476.37 - lr: 0.100000\n",
            "2022-03-03 15:55:41,804 epoch 10 - iter 11/19 - loss 0.95593752 - samples/sec: 484.23 - lr: 0.100000\n",
            "2022-03-03 15:55:41,876 epoch 10 - iter 12/19 - loss 0.94247316 - samples/sec: 496.63 - lr: 0.100000\n",
            "2022-03-03 15:55:41,946 epoch 10 - iter 13/19 - loss 0.95481441 - samples/sec: 467.29 - lr: 0.100000\n",
            "2022-03-03 15:55:42,015 epoch 10 - iter 14/19 - loss 0.95670652 - samples/sec: 478.40 - lr: 0.100000\n",
            "2022-03-03 15:55:42,082 epoch 10 - iter 15/19 - loss 0.94696938 - samples/sec: 536.86 - lr: 0.100000\n",
            "2022-03-03 15:55:42,159 epoch 10 - iter 16/19 - loss 0.94719248 - samples/sec: 504.76 - lr: 0.100000\n",
            "2022-03-03 15:55:42,231 epoch 10 - iter 17/19 - loss 0.95209757 - samples/sec: 456.37 - lr: 0.100000\n",
            "2022-03-03 15:55:42,300 epoch 10 - iter 18/19 - loss 0.94767906 - samples/sec: 480.72 - lr: 0.100000\n",
            "2022-03-03 15:55:42,355 epoch 10 - iter 19/19 - loss 0.94864525 - samples/sec: 753.35 - lr: 0.100000\n",
            "2022-03-03 15:55:42,824 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:55:42,825 EPOCH 10 done: loss 0.9486 - lr 0.1000000\n",
            "2022-03-03 15:55:44,195 DEV : loss 1.2704132795333862 - score 0.3485\n",
            "2022-03-03 15:55:44,267 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 15:55:44,274 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:55:45,193 epoch 11 - iter 1/19 - loss 1.07517111 - samples/sec: 171.92 - lr: 0.100000\n",
            "2022-03-03 15:55:45,280 epoch 11 - iter 2/19 - loss 0.97815603 - samples/sec: 467.90 - lr: 0.100000\n",
            "2022-03-03 15:55:45,358 epoch 11 - iter 3/19 - loss 0.97447439 - samples/sec: 423.53 - lr: 0.100000\n",
            "2022-03-03 15:55:45,445 epoch 11 - iter 4/19 - loss 1.00219101 - samples/sec: 446.54 - lr: 0.100000\n",
            "2022-03-03 15:55:45,548 epoch 11 - iter 5/19 - loss 1.00511675 - samples/sec: 478.69 - lr: 0.100000\n",
            "2022-03-03 15:55:45,619 epoch 11 - iter 6/19 - loss 0.98924309 - samples/sec: 460.35 - lr: 0.100000\n",
            "2022-03-03 15:55:45,689 epoch 11 - iter 7/19 - loss 0.98540942 - samples/sec: 471.75 - lr: 0.100000\n",
            "2022-03-03 15:55:45,759 epoch 11 - iter 8/19 - loss 0.97353139 - samples/sec: 477.18 - lr: 0.100000\n",
            "2022-03-03 15:55:45,823 epoch 11 - iter 9/19 - loss 0.96614331 - samples/sec: 510.33 - lr: 0.100000\n",
            "2022-03-03 15:55:45,933 epoch 11 - iter 10/19 - loss 0.95975037 - samples/sec: 432.43 - lr: 0.100000\n",
            "2022-03-03 15:55:45,999 epoch 11 - iter 11/19 - loss 0.95812556 - samples/sec: 501.05 - lr: 0.100000\n",
            "2022-03-03 15:55:46,064 epoch 11 - iter 12/19 - loss 0.95717859 - samples/sec: 512.76 - lr: 0.100000\n",
            "2022-03-03 15:55:46,127 epoch 11 - iter 13/19 - loss 0.96354896 - samples/sec: 530.99 - lr: 0.100000\n",
            "2022-03-03 15:55:46,197 epoch 11 - iter 14/19 - loss 0.95972430 - samples/sec: 474.33 - lr: 0.100000\n",
            "2022-03-03 15:55:46,276 epoch 11 - iter 15/19 - loss 0.95553785 - samples/sec: 447.47 - lr: 0.100000\n",
            "2022-03-03 15:55:46,347 epoch 11 - iter 16/19 - loss 0.94794481 - samples/sec: 514.48 - lr: 0.100000\n",
            "2022-03-03 15:55:46,419 epoch 11 - iter 17/19 - loss 0.94903440 - samples/sec: 459.44 - lr: 0.100000\n",
            "2022-03-03 15:55:46,487 epoch 11 - iter 18/19 - loss 0.94683651 - samples/sec: 482.16 - lr: 0.100000\n",
            "2022-03-03 15:55:46,542 epoch 11 - iter 19/19 - loss 0.94385659 - samples/sec: 601.40 - lr: 0.100000\n",
            "2022-03-03 15:55:47,028 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:55:47,029 EPOCH 11 done: loss 0.9439 - lr 0.1000000\n",
            "2022-03-03 15:55:48,383 DEV : loss 1.2664158344268799 - score 0.3636\n",
            "2022-03-03 15:55:48,464 BAD EPOCHS (no improvement): 2\n",
            "2022-03-03 15:55:48,466 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:55:49,444 epoch 12 - iter 1/19 - loss 0.83955860 - samples/sec: 149.51 - lr: 0.100000\n",
            "2022-03-03 15:55:49,537 epoch 12 - iter 2/19 - loss 0.89009359 - samples/sec: 474.32 - lr: 0.100000\n",
            "2022-03-03 15:55:49,626 epoch 12 - iter 3/19 - loss 0.91213969 - samples/sec: 453.94 - lr: 0.100000\n",
            "2022-03-03 15:55:49,699 epoch 12 - iter 4/19 - loss 0.92583275 - samples/sec: 452.98 - lr: 0.100000\n",
            "2022-03-03 15:55:49,772 epoch 12 - iter 5/19 - loss 0.93272872 - samples/sec: 491.75 - lr: 0.100000\n",
            "2022-03-03 15:55:49,876 epoch 12 - iter 6/19 - loss 0.89822858 - samples/sec: 378.75 - lr: 0.100000\n",
            "2022-03-03 15:55:49,959 epoch 12 - iter 7/19 - loss 0.88374840 - samples/sec: 462.55 - lr: 0.100000\n",
            "2022-03-03 15:55:50,034 epoch 12 - iter 8/19 - loss 0.89823250 - samples/sec: 535.89 - lr: 0.100000\n",
            "2022-03-03 15:55:50,121 epoch 12 - iter 9/19 - loss 0.90971217 - samples/sec: 433.76 - lr: 0.100000\n",
            "2022-03-03 15:55:50,206 epoch 12 - iter 10/19 - loss 0.90558016 - samples/sec: 516.62 - lr: 0.100000\n",
            "2022-03-03 15:55:50,274 epoch 12 - iter 11/19 - loss 0.90272161 - samples/sec: 483.57 - lr: 0.100000\n",
            "2022-03-03 15:55:50,337 epoch 12 - iter 12/19 - loss 0.90886075 - samples/sec: 533.62 - lr: 0.100000\n",
            "2022-03-03 15:55:50,405 epoch 12 - iter 13/19 - loss 0.92204847 - samples/sec: 490.12 - lr: 0.100000\n",
            "2022-03-03 15:55:50,466 epoch 12 - iter 14/19 - loss 0.92945740 - samples/sec: 543.42 - lr: 0.100000\n",
            "2022-03-03 15:55:50,539 epoch 12 - iter 15/19 - loss 0.93578694 - samples/sec: 553.21 - lr: 0.100000\n",
            "2022-03-03 15:55:50,611 epoch 12 - iter 16/19 - loss 0.94000897 - samples/sec: 527.56 - lr: 0.100000\n",
            "2022-03-03 15:55:50,681 epoch 12 - iter 17/19 - loss 0.94259022 - samples/sec: 472.98 - lr: 0.100000\n",
            "2022-03-03 15:55:50,751 epoch 12 - iter 18/19 - loss 0.94083355 - samples/sec: 473.16 - lr: 0.100000\n",
            "2022-03-03 15:55:50,807 epoch 12 - iter 19/19 - loss 0.93972541 - samples/sec: 587.50 - lr: 0.100000\n",
            "2022-03-03 15:55:51,268 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:55:51,275 EPOCH 12 done: loss 0.9397 - lr 0.1000000\n",
            "2022-03-03 15:55:52,593 DEV : loss 1.2114628553390503 - score 0.3485\n",
            "2022-03-03 15:55:52,667 BAD EPOCHS (no improvement): 3\n",
            "2022-03-03 15:55:52,669 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:55:53,614 epoch 13 - iter 1/19 - loss 0.78757381 - samples/sec: 197.78 - lr: 0.100000\n",
            "2022-03-03 15:55:53,715 epoch 13 - iter 2/19 - loss 0.84013033 - samples/sec: 438.70 - lr: 0.100000\n",
            "2022-03-03 15:55:53,792 epoch 13 - iter 3/19 - loss 0.84916641 - samples/sec: 474.01 - lr: 0.100000\n",
            "2022-03-03 15:55:53,866 epoch 13 - iter 4/19 - loss 0.86845826 - samples/sec: 460.19 - lr: 0.100000\n",
            "2022-03-03 15:55:53,962 epoch 13 - iter 5/19 - loss 0.90096344 - samples/sec: 368.44 - lr: 0.100000\n",
            "2022-03-03 15:55:54,071 epoch 13 - iter 6/19 - loss 0.88602085 - samples/sec: 451.56 - lr: 0.100000\n",
            "2022-03-03 15:55:54,147 epoch 13 - iter 7/19 - loss 0.88785787 - samples/sec: 460.32 - lr: 0.100000\n",
            "2022-03-03 15:55:54,215 epoch 13 - iter 8/19 - loss 0.89065243 - samples/sec: 485.68 - lr: 0.100000\n",
            "2022-03-03 15:55:54,289 epoch 13 - iter 9/19 - loss 0.88546550 - samples/sec: 571.18 - lr: 0.100000\n",
            "2022-03-03 15:55:54,357 epoch 13 - iter 10/19 - loss 0.90183899 - samples/sec: 488.00 - lr: 0.100000\n",
            "2022-03-03 15:55:54,434 epoch 13 - iter 11/19 - loss 0.91452576 - samples/sec: 451.87 - lr: 0.100000\n",
            "2022-03-03 15:55:54,507 epoch 13 - iter 12/19 - loss 0.91521287 - samples/sec: 453.26 - lr: 0.100000\n",
            "2022-03-03 15:55:54,573 epoch 13 - iter 13/19 - loss 0.92041434 - samples/sec: 531.67 - lr: 0.100000\n",
            "2022-03-03 15:55:54,640 epoch 13 - iter 14/19 - loss 0.92817096 - samples/sec: 503.36 - lr: 0.100000\n",
            "2022-03-03 15:55:54,706 epoch 13 - iter 15/19 - loss 0.92568200 - samples/sec: 550.54 - lr: 0.100000\n",
            "2022-03-03 15:55:54,771 epoch 13 - iter 16/19 - loss 0.92368719 - samples/sec: 503.63 - lr: 0.100000\n",
            "2022-03-03 15:55:54,845 epoch 13 - iter 17/19 - loss 0.92696864 - samples/sec: 582.55 - lr: 0.100000\n",
            "2022-03-03 15:55:54,910 epoch 13 - iter 18/19 - loss 0.93280367 - samples/sec: 526.30 - lr: 0.100000\n",
            "2022-03-03 15:55:54,963 epoch 13 - iter 19/19 - loss 0.92929835 - samples/sec: 699.18 - lr: 0.100000\n",
            "2022-03-03 15:55:55,419 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:55:55,421 EPOCH 13 done: loss 0.9293 - lr 0.1000000\n",
            "2022-03-03 15:55:56,730 DEV : loss 1.2053146362304688 - score 0.3485\n",
            "Epoch    13: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2022-03-03 15:55:56,805 BAD EPOCHS (no improvement): 4\n",
            "2022-03-03 15:55:56,807 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:55:57,736 epoch 14 - iter 1/19 - loss 0.95271337 - samples/sec: 141.72 - lr: 0.050000\n",
            "2022-03-03 15:55:57,861 epoch 14 - iter 2/19 - loss 0.94825515 - samples/sec: 456.81 - lr: 0.050000\n",
            "2022-03-03 15:55:57,935 epoch 14 - iter 3/19 - loss 0.90574984 - samples/sec: 467.06 - lr: 0.050000\n",
            "2022-03-03 15:55:58,005 epoch 14 - iter 4/19 - loss 0.90395127 - samples/sec: 505.66 - lr: 0.050000\n",
            "2022-03-03 15:55:58,070 epoch 14 - iter 5/19 - loss 0.90713949 - samples/sec: 517.78 - lr: 0.050000\n",
            "2022-03-03 15:55:58,136 epoch 14 - iter 6/19 - loss 0.90857768 - samples/sec: 505.18 - lr: 0.050000\n",
            "2022-03-03 15:55:58,205 epoch 14 - iter 7/19 - loss 0.92712467 - samples/sec: 504.81 - lr: 0.050000\n",
            "2022-03-03 15:55:58,278 epoch 14 - iter 8/19 - loss 0.94501254 - samples/sec: 536.35 - lr: 0.050000\n",
            "2022-03-03 15:55:58,343 epoch 14 - iter 9/19 - loss 0.93740474 - samples/sec: 508.12 - lr: 0.050000\n",
            "2022-03-03 15:55:58,415 epoch 14 - iter 10/19 - loss 0.94056169 - samples/sec: 460.61 - lr: 0.050000\n",
            "2022-03-03 15:55:58,484 epoch 14 - iter 11/19 - loss 0.94246972 - samples/sec: 473.28 - lr: 0.050000\n",
            "2022-03-03 15:55:58,553 epoch 14 - iter 12/19 - loss 0.93823957 - samples/sec: 489.96 - lr: 0.050000\n",
            "2022-03-03 15:55:58,622 epoch 14 - iter 13/19 - loss 0.94075197 - samples/sec: 484.97 - lr: 0.050000\n",
            "2022-03-03 15:55:58,698 epoch 14 - iter 14/19 - loss 0.93764834 - samples/sec: 524.36 - lr: 0.050000\n",
            "2022-03-03 15:55:58,770 epoch 14 - iter 15/19 - loss 0.93815478 - samples/sec: 534.69 - lr: 0.050000\n",
            "2022-03-03 15:55:58,839 epoch 14 - iter 16/19 - loss 0.93933767 - samples/sec: 533.41 - lr: 0.050000\n",
            "2022-03-03 15:55:58,906 epoch 14 - iter 17/19 - loss 0.93322514 - samples/sec: 557.01 - lr: 0.050000\n",
            "2022-03-03 15:55:58,975 epoch 14 - iter 18/19 - loss 0.92132059 - samples/sec: 542.03 - lr: 0.050000\n",
            "2022-03-03 15:55:59,029 epoch 14 - iter 19/19 - loss 0.92634662 - samples/sec: 666.43 - lr: 0.050000\n",
            "2022-03-03 15:55:59,496 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:55:59,499 EPOCH 14 done: loss 0.9263 - lr 0.0500000\n",
            "2022-03-03 15:56:00,855 DEV : loss 1.2317249774932861 - score 0.3636\n",
            "2022-03-03 15:56:00,932 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 15:56:00,935 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:01,870 epoch 15 - iter 1/19 - loss 0.87894851 - samples/sec: 190.27 - lr: 0.050000\n",
            "2022-03-03 15:56:01,968 epoch 15 - iter 2/19 - loss 0.98582223 - samples/sec: 372.56 - lr: 0.050000\n",
            "2022-03-03 15:56:03,578 epoch 15 - iter 3/19 - loss 0.96542350 - samples/sec: 514.41 - lr: 0.050000\n",
            "2022-03-03 15:56:03,666 epoch 15 - iter 4/19 - loss 0.96652602 - samples/sec: 533.27 - lr: 0.050000\n",
            "2022-03-03 15:56:03,737 epoch 15 - iter 5/19 - loss 0.96905258 - samples/sec: 496.09 - lr: 0.050000\n",
            "2022-03-03 15:56:03,817 epoch 15 - iter 6/19 - loss 0.96925672 - samples/sec: 412.89 - lr: 0.050000\n",
            "2022-03-03 15:56:03,890 epoch 15 - iter 7/19 - loss 0.95151610 - samples/sec: 531.47 - lr: 0.050000\n",
            "2022-03-03 15:56:03,956 epoch 15 - iter 8/19 - loss 0.94560049 - samples/sec: 497.47 - lr: 0.050000\n",
            "2022-03-03 15:56:04,032 epoch 15 - iter 9/19 - loss 0.93717688 - samples/sec: 531.22 - lr: 0.050000\n",
            "2022-03-03 15:56:04,113 epoch 15 - iter 10/19 - loss 0.93531514 - samples/sec: 534.04 - lr: 0.050000\n",
            "2022-03-03 15:56:04,175 epoch 15 - iter 11/19 - loss 0.93794867 - samples/sec: 528.98 - lr: 0.050000\n",
            "2022-03-03 15:56:04,242 epoch 15 - iter 12/19 - loss 0.93795763 - samples/sec: 490.96 - lr: 0.050000\n",
            "2022-03-03 15:56:04,301 epoch 15 - iter 13/19 - loss 0.92884796 - samples/sec: 560.35 - lr: 0.050000\n",
            "2022-03-03 15:56:04,370 epoch 15 - iter 14/19 - loss 0.93095932 - samples/sec: 478.08 - lr: 0.050000\n",
            "2022-03-03 15:56:04,434 epoch 15 - iter 15/19 - loss 0.92614399 - samples/sec: 552.44 - lr: 0.050000\n",
            "2022-03-03 15:56:04,508 epoch 15 - iter 16/19 - loss 0.91464562 - samples/sec: 515.17 - lr: 0.050000\n",
            "2022-03-03 15:56:04,569 epoch 15 - iter 17/19 - loss 0.91893730 - samples/sec: 539.60 - lr: 0.050000\n",
            "2022-03-03 15:56:04,643 epoch 15 - iter 18/19 - loss 0.91305528 - samples/sec: 447.62 - lr: 0.050000\n",
            "2022-03-03 15:56:04,693 epoch 15 - iter 19/19 - loss 0.91787613 - samples/sec: 666.66 - lr: 0.050000\n",
            "2022-03-03 15:56:05,133 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:05,136 EPOCH 15 done: loss 0.9179 - lr 0.0500000\n",
            "2022-03-03 15:56:06,448 DEV : loss 1.2290961742401123 - score 0.3636\n",
            "2022-03-03 15:56:06,522 BAD EPOCHS (no improvement): 2\n",
            "2022-03-03 15:56:06,525 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:07,466 epoch 16 - iter 1/19 - loss 0.88944316 - samples/sec: 145.66 - lr: 0.050000\n",
            "2022-03-03 15:56:07,550 epoch 16 - iter 2/19 - loss 0.96919656 - samples/sec: 470.23 - lr: 0.050000\n",
            "2022-03-03 15:56:07,639 epoch 16 - iter 3/19 - loss 0.91543978 - samples/sec: 453.83 - lr: 0.050000\n",
            "2022-03-03 15:56:07,724 epoch 16 - iter 4/19 - loss 0.90721123 - samples/sec: 471.23 - lr: 0.050000\n",
            "2022-03-03 15:56:07,801 epoch 16 - iter 5/19 - loss 0.92796835 - samples/sec: 429.62 - lr: 0.050000\n",
            "2022-03-03 15:56:07,877 epoch 16 - iter 6/19 - loss 0.92708440 - samples/sec: 495.54 - lr: 0.050000\n",
            "2022-03-03 15:56:07,952 epoch 16 - iter 7/19 - loss 0.92590588 - samples/sec: 454.13 - lr: 0.050000\n",
            "2022-03-03 15:56:08,033 epoch 16 - iter 8/19 - loss 0.92587163 - samples/sec: 519.97 - lr: 0.050000\n",
            "2022-03-03 15:56:08,107 epoch 16 - iter 9/19 - loss 0.90689527 - samples/sec: 547.81 - lr: 0.050000\n",
            "2022-03-03 15:56:08,181 epoch 16 - iter 10/19 - loss 0.90279726 - samples/sec: 572.51 - lr: 0.050000\n",
            "2022-03-03 15:56:08,252 epoch 16 - iter 11/19 - loss 0.90326255 - samples/sec: 465.78 - lr: 0.050000\n",
            "2022-03-03 15:56:08,319 epoch 16 - iter 12/19 - loss 0.90367530 - samples/sec: 544.15 - lr: 0.050000\n",
            "2022-03-03 15:56:08,388 epoch 16 - iter 13/19 - loss 0.90230809 - samples/sec: 474.18 - lr: 0.050000\n",
            "2022-03-03 15:56:08,470 epoch 16 - iter 14/19 - loss 0.90626602 - samples/sec: 547.98 - lr: 0.050000\n",
            "2022-03-03 15:56:08,540 epoch 16 - iter 15/19 - loss 0.90492626 - samples/sec: 468.03 - lr: 0.050000\n",
            "2022-03-03 15:56:08,613 epoch 16 - iter 16/19 - loss 0.90925011 - samples/sec: 448.63 - lr: 0.050000\n",
            "2022-03-03 15:56:08,690 epoch 16 - iter 17/19 - loss 0.91244459 - samples/sec: 437.39 - lr: 0.050000\n",
            "2022-03-03 15:56:08,750 epoch 16 - iter 18/19 - loss 0.90633460 - samples/sec: 545.05 - lr: 0.050000\n",
            "2022-03-03 15:56:08,804 epoch 16 - iter 19/19 - loss 0.90504578 - samples/sec: 623.39 - lr: 0.050000\n",
            "2022-03-03 15:56:09,236 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:09,242 EPOCH 16 done: loss 0.9050 - lr 0.0500000\n",
            "2022-03-03 15:56:10,558 DEV : loss 1.2123174667358398 - score 0.3485\n",
            "2022-03-03 15:56:10,630 BAD EPOCHS (no improvement): 3\n",
            "2022-03-03 15:56:10,632 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:11,578 epoch 17 - iter 1/19 - loss 0.95096815 - samples/sec: 180.20 - lr: 0.050000\n",
            "2022-03-03 15:56:11,667 epoch 17 - iter 2/19 - loss 0.87521097 - samples/sec: 465.94 - lr: 0.050000\n",
            "2022-03-03 15:56:11,742 epoch 17 - iter 3/19 - loss 0.86174774 - samples/sec: 435.02 - lr: 0.050000\n",
            "2022-03-03 15:56:11,817 epoch 17 - iter 4/19 - loss 0.89541556 - samples/sec: 508.76 - lr: 0.050000\n",
            "2022-03-03 15:56:11,904 epoch 17 - iter 5/19 - loss 0.91386684 - samples/sec: 509.25 - lr: 0.050000\n",
            "2022-03-03 15:56:11,981 epoch 17 - iter 6/19 - loss 0.91313201 - samples/sec: 478.47 - lr: 0.050000\n",
            "2022-03-03 15:56:12,066 epoch 17 - iter 7/19 - loss 0.92716832 - samples/sec: 418.60 - lr: 0.050000\n",
            "2022-03-03 15:56:12,167 epoch 17 - iter 8/19 - loss 0.92482360 - samples/sec: 546.71 - lr: 0.050000\n",
            "2022-03-03 15:56:12,239 epoch 17 - iter 9/19 - loss 0.90573770 - samples/sec: 462.38 - lr: 0.050000\n",
            "2022-03-03 15:56:12,306 epoch 17 - iter 10/19 - loss 0.91072218 - samples/sec: 493.32 - lr: 0.050000\n",
            "2022-03-03 15:56:12,375 epoch 17 - iter 11/19 - loss 0.90809939 - samples/sec: 482.54 - lr: 0.050000\n",
            "2022-03-03 15:56:12,448 epoch 17 - iter 12/19 - loss 0.90585186 - samples/sec: 450.38 - lr: 0.050000\n",
            "2022-03-03 15:56:12,517 epoch 17 - iter 13/19 - loss 0.89485977 - samples/sec: 474.49 - lr: 0.050000\n",
            "2022-03-03 15:56:12,588 epoch 17 - iter 14/19 - loss 0.88848530 - samples/sec: 477.87 - lr: 0.050000\n",
            "2022-03-03 15:56:12,658 epoch 17 - iter 15/19 - loss 0.89784440 - samples/sec: 472.47 - lr: 0.050000\n",
            "2022-03-03 15:56:12,728 epoch 17 - iter 16/19 - loss 0.90115838 - samples/sec: 471.49 - lr: 0.050000\n",
            "2022-03-03 15:56:12,792 epoch 17 - iter 17/19 - loss 0.90443333 - samples/sec: 511.56 - lr: 0.050000\n",
            "2022-03-03 15:56:12,863 epoch 17 - iter 18/19 - loss 0.89701492 - samples/sec: 466.47 - lr: 0.050000\n",
            "2022-03-03 15:56:12,911 epoch 17 - iter 19/19 - loss 0.89228938 - samples/sec: 681.84 - lr: 0.050000\n",
            "2022-03-03 15:56:13,367 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:13,374 EPOCH 17 done: loss 0.8923 - lr 0.0500000\n",
            "2022-03-03 15:56:14,659 DEV : loss 1.268858551979065 - score 0.3788\n",
            "Epoch    17: reducing learning rate of group 0 to 2.5000e-02.\n",
            "2022-03-03 15:56:14,736 BAD EPOCHS (no improvement): 4\n",
            "2022-03-03 15:56:14,738 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:15,677 epoch 18 - iter 1/19 - loss 1.00214231 - samples/sec: 140.62 - lr: 0.025000\n",
            "2022-03-03 15:56:15,782 epoch 18 - iter 2/19 - loss 0.98515716 - samples/sec: 474.21 - lr: 0.025000\n",
            "2022-03-03 15:56:15,862 epoch 18 - iter 3/19 - loss 0.97678111 - samples/sec: 438.33 - lr: 0.025000\n",
            "2022-03-03 15:56:15,929 epoch 18 - iter 4/19 - loss 0.95892704 - samples/sec: 493.84 - lr: 0.025000\n",
            "2022-03-03 15:56:16,004 epoch 18 - iter 5/19 - loss 0.95212444 - samples/sec: 467.04 - lr: 0.025000\n",
            "2022-03-03 15:56:16,091 epoch 18 - iter 6/19 - loss 0.94085644 - samples/sec: 467.69 - lr: 0.025000\n",
            "2022-03-03 15:56:16,166 epoch 18 - iter 7/19 - loss 0.93919556 - samples/sec: 472.12 - lr: 0.025000\n",
            "2022-03-03 15:56:16,264 epoch 18 - iter 8/19 - loss 0.93340968 - samples/sec: 573.48 - lr: 0.025000\n",
            "2022-03-03 15:56:16,332 epoch 18 - iter 9/19 - loss 0.94527690 - samples/sec: 484.45 - lr: 0.025000\n",
            "2022-03-03 15:56:16,404 epoch 18 - iter 10/19 - loss 0.93788935 - samples/sec: 456.32 - lr: 0.025000\n",
            "2022-03-03 15:56:16,476 epoch 18 - iter 11/19 - loss 0.93814488 - samples/sec: 465.54 - lr: 0.025000\n",
            "2022-03-03 15:56:16,545 epoch 18 - iter 12/19 - loss 0.94817852 - samples/sec: 476.31 - lr: 0.025000\n",
            "2022-03-03 15:56:16,614 epoch 18 - iter 13/19 - loss 0.94986670 - samples/sec: 522.26 - lr: 0.025000\n",
            "2022-03-03 15:56:16,686 epoch 18 - iter 14/19 - loss 0.95163236 - samples/sec: 490.09 - lr: 0.025000\n",
            "2022-03-03 15:56:16,759 epoch 18 - iter 15/19 - loss 0.94636499 - samples/sec: 483.21 - lr: 0.025000\n",
            "2022-03-03 15:56:16,827 epoch 18 - iter 16/19 - loss 0.93848923 - samples/sec: 573.80 - lr: 0.025000\n",
            "2022-03-03 15:56:16,892 epoch 18 - iter 17/19 - loss 0.92326227 - samples/sec: 506.04 - lr: 0.025000\n",
            "2022-03-03 15:56:16,957 epoch 18 - iter 18/19 - loss 0.91109498 - samples/sec: 507.92 - lr: 0.025000\n",
            "2022-03-03 15:56:17,008 epoch 18 - iter 19/19 - loss 0.90711426 - samples/sec: 655.09 - lr: 0.025000\n",
            "2022-03-03 15:56:17,445 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:17,447 EPOCH 18 done: loss 0.9071 - lr 0.0250000\n",
            "2022-03-03 15:56:18,806 DEV : loss 1.23180091381073 - score 0.3636\n",
            "2022-03-03 15:56:18,885 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 15:56:18,887 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:19,836 epoch 19 - iter 1/19 - loss 0.83579415 - samples/sec: 229.40 - lr: 0.025000\n",
            "2022-03-03 15:56:19,921 epoch 19 - iter 2/19 - loss 0.81910527 - samples/sec: 449.96 - lr: 0.025000\n",
            "2022-03-03 15:56:20,016 epoch 19 - iter 3/19 - loss 0.86699575 - samples/sec: 468.19 - lr: 0.025000\n",
            "2022-03-03 15:56:20,092 epoch 19 - iter 4/19 - loss 0.83756216 - samples/sec: 445.15 - lr: 0.025000\n",
            "2022-03-03 15:56:20,162 epoch 19 - iter 5/19 - loss 0.85552046 - samples/sec: 470.16 - lr: 0.025000\n",
            "2022-03-03 15:56:20,242 epoch 19 - iter 6/19 - loss 0.87792692 - samples/sec: 465.10 - lr: 0.025000\n",
            "2022-03-03 15:56:20,338 epoch 19 - iter 7/19 - loss 0.87630773 - samples/sec: 520.00 - lr: 0.025000\n",
            "2022-03-03 15:56:20,407 epoch 19 - iter 8/19 - loss 0.88591005 - samples/sec: 481.64 - lr: 0.025000\n",
            "2022-03-03 15:56:20,503 epoch 19 - iter 9/19 - loss 0.86859288 - samples/sec: 341.31 - lr: 0.025000\n",
            "2022-03-03 15:56:20,572 epoch 19 - iter 10/19 - loss 0.85351285 - samples/sec: 477.46 - lr: 0.025000\n",
            "2022-03-03 15:56:20,644 epoch 19 - iter 11/19 - loss 0.85998285 - samples/sec: 462.83 - lr: 0.025000\n",
            "2022-03-03 15:56:20,719 epoch 19 - iter 12/19 - loss 0.87081345 - samples/sec: 443.16 - lr: 0.025000\n",
            "2022-03-03 15:56:20,785 epoch 19 - iter 13/19 - loss 0.87806138 - samples/sec: 523.73 - lr: 0.025000\n",
            "2022-03-03 15:56:20,851 epoch 19 - iter 14/19 - loss 0.87729486 - samples/sec: 498.60 - lr: 0.025000\n",
            "2022-03-03 15:56:20,926 epoch 19 - iter 15/19 - loss 0.88974603 - samples/sec: 569.03 - lr: 0.025000\n",
            "2022-03-03 15:56:20,993 epoch 19 - iter 16/19 - loss 0.88309104 - samples/sec: 492.13 - lr: 0.025000\n",
            "2022-03-03 15:56:21,063 epoch 19 - iter 17/19 - loss 0.89470641 - samples/sec: 474.84 - lr: 0.025000\n",
            "2022-03-03 15:56:21,128 epoch 19 - iter 18/19 - loss 0.90219992 - samples/sec: 536.20 - lr: 0.025000\n",
            "2022-03-03 15:56:21,184 epoch 19 - iter 19/19 - loss 0.90286403 - samples/sec: 647.53 - lr: 0.025000\n",
            "2022-03-03 15:56:21,639 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:21,641 EPOCH 19 done: loss 0.9029 - lr 0.0250000\n",
            "2022-03-03 15:56:22,944 DEV : loss 1.2090182304382324 - score 0.3636\n",
            "2022-03-03 15:56:23,016 BAD EPOCHS (no improvement): 2\n",
            "2022-03-03 15:56:23,019 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:23,967 epoch 20 - iter 1/19 - loss 0.86220735 - samples/sec: 305.79 - lr: 0.025000\n",
            "2022-03-03 15:56:24,052 epoch 20 - iter 2/19 - loss 0.89600232 - samples/sec: 399.95 - lr: 0.025000\n",
            "2022-03-03 15:56:24,125 epoch 20 - iter 3/19 - loss 0.90772518 - samples/sec: 495.99 - lr: 0.025000\n",
            "2022-03-03 15:56:24,194 epoch 20 - iter 4/19 - loss 0.87704225 - samples/sec: 479.09 - lr: 0.025000\n",
            "2022-03-03 15:56:24,279 epoch 20 - iter 5/19 - loss 0.87221047 - samples/sec: 503.42 - lr: 0.025000\n",
            "2022-03-03 15:56:24,361 epoch 20 - iter 6/19 - loss 0.86316184 - samples/sec: 399.71 - lr: 0.025000\n",
            "2022-03-03 15:56:24,472 epoch 20 - iter 7/19 - loss 0.86109543 - samples/sec: 484.52 - lr: 0.025000\n",
            "2022-03-03 15:56:24,538 epoch 20 - iter 8/19 - loss 0.87023970 - samples/sec: 500.46 - lr: 0.025000\n",
            "2022-03-03 15:56:24,608 epoch 20 - iter 9/19 - loss 0.87618624 - samples/sec: 468.89 - lr: 0.025000\n",
            "2022-03-03 15:56:24,681 epoch 20 - iter 10/19 - loss 0.86443682 - samples/sec: 448.09 - lr: 0.025000\n",
            "2022-03-03 15:56:24,744 epoch 20 - iter 11/19 - loss 0.86830410 - samples/sec: 523.58 - lr: 0.025000\n",
            "2022-03-03 15:56:24,810 epoch 20 - iter 12/19 - loss 0.87017751 - samples/sec: 552.63 - lr: 0.025000\n",
            "2022-03-03 15:56:24,880 epoch 20 - iter 13/19 - loss 0.87369102 - samples/sec: 580.87 - lr: 0.025000\n",
            "2022-03-03 15:56:24,946 epoch 20 - iter 14/19 - loss 0.87361409 - samples/sec: 543.61 - lr: 0.025000\n",
            "2022-03-03 15:56:25,018 epoch 20 - iter 15/19 - loss 0.87432607 - samples/sec: 460.85 - lr: 0.025000\n",
            "2022-03-03 15:56:25,098 epoch 20 - iter 16/19 - loss 0.87723563 - samples/sec: 454.17 - lr: 0.025000\n",
            "2022-03-03 15:56:25,164 epoch 20 - iter 17/19 - loss 0.87639026 - samples/sec: 498.74 - lr: 0.025000\n",
            "2022-03-03 15:56:25,232 epoch 20 - iter 18/19 - loss 0.89335303 - samples/sec: 484.85 - lr: 0.025000\n",
            "2022-03-03 15:56:25,286 epoch 20 - iter 19/19 - loss 0.89197460 - samples/sec: 612.05 - lr: 0.025000\n",
            "2022-03-03 15:56:25,735 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:25,737 EPOCH 20 done: loss 0.8920 - lr 0.0250000\n",
            "2022-03-03 15:56:27,028 DEV : loss 1.207627534866333 - score 0.3485\n",
            "2022-03-03 15:56:27,101 BAD EPOCHS (no improvement): 3\n",
            "2022-03-03 15:56:27,105 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:28,035 epoch 21 - iter 1/19 - loss 0.93614757 - samples/sec: 179.58 - lr: 0.025000\n",
            "2022-03-03 15:56:28,136 epoch 21 - iter 2/19 - loss 0.94437125 - samples/sec: 463.18 - lr: 0.025000\n",
            "2022-03-03 15:56:28,211 epoch 21 - iter 3/19 - loss 0.90096843 - samples/sec: 469.30 - lr: 0.025000\n",
            "2022-03-03 15:56:28,283 epoch 21 - iter 4/19 - loss 0.90137661 - samples/sec: 456.91 - lr: 0.025000\n",
            "2022-03-03 15:56:28,354 epoch 21 - iter 5/19 - loss 0.91002202 - samples/sec: 519.92 - lr: 0.025000\n",
            "2022-03-03 15:56:28,455 epoch 21 - iter 6/19 - loss 0.90636569 - samples/sec: 503.22 - lr: 0.025000\n",
            "2022-03-03 15:56:28,528 epoch 21 - iter 7/19 - loss 0.90398771 - samples/sec: 458.91 - lr: 0.025000\n",
            "2022-03-03 15:56:28,597 epoch 21 - iter 8/19 - loss 0.92362709 - samples/sec: 477.53 - lr: 0.025000\n",
            "2022-03-03 15:56:28,667 epoch 21 - iter 9/19 - loss 0.92833567 - samples/sec: 508.06 - lr: 0.025000\n",
            "2022-03-03 15:56:28,738 epoch 21 - iter 10/19 - loss 0.93106043 - samples/sec: 641.35 - lr: 0.025000\n",
            "2022-03-03 15:56:28,805 epoch 21 - iter 11/19 - loss 0.92307930 - samples/sec: 513.76 - lr: 0.025000\n",
            "2022-03-03 15:56:28,869 epoch 21 - iter 12/19 - loss 0.92415270 - samples/sec: 512.39 - lr: 0.025000\n",
            "2022-03-03 15:56:28,933 epoch 21 - iter 13/19 - loss 0.91721804 - samples/sec: 511.01 - lr: 0.025000\n",
            "2022-03-03 15:56:29,009 epoch 21 - iter 14/19 - loss 0.91225622 - samples/sec: 501.35 - lr: 0.025000\n",
            "2022-03-03 15:56:29,077 epoch 21 - iter 15/19 - loss 0.90390668 - samples/sec: 483.65 - lr: 0.025000\n",
            "2022-03-03 15:56:29,153 epoch 21 - iter 16/19 - loss 0.89874732 - samples/sec: 539.41 - lr: 0.025000\n",
            "2022-03-03 15:56:29,224 epoch 21 - iter 17/19 - loss 0.89761270 - samples/sec: 463.94 - lr: 0.025000\n",
            "2022-03-03 15:56:29,295 epoch 21 - iter 18/19 - loss 0.88914827 - samples/sec: 462.29 - lr: 0.025000\n",
            "2022-03-03 15:56:29,346 epoch 21 - iter 19/19 - loss 0.89218921 - samples/sec: 648.72 - lr: 0.025000\n",
            "2022-03-03 15:56:29,803 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:29,811 EPOCH 21 done: loss 0.8922 - lr 0.0250000\n",
            "2022-03-03 15:56:31,104 DEV : loss 1.2254174947738647 - score 0.3636\n",
            "Epoch    21: reducing learning rate of group 0 to 1.2500e-02.\n",
            "2022-03-03 15:56:31,175 BAD EPOCHS (no improvement): 4\n",
            "2022-03-03 15:56:31,181 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:32,142 epoch 22 - iter 1/19 - loss 0.87462223 - samples/sec: 194.51 - lr: 0.012500\n",
            "2022-03-03 15:56:33,718 epoch 22 - iter 2/19 - loss 0.86010167 - samples/sec: 399.77 - lr: 0.012500\n",
            "2022-03-03 15:56:33,819 epoch 22 - iter 3/19 - loss 0.92257557 - samples/sec: 390.76 - lr: 0.012500\n",
            "2022-03-03 15:56:33,931 epoch 22 - iter 4/19 - loss 0.89618927 - samples/sec: 358.30 - lr: 0.012500\n",
            "2022-03-03 15:56:34,010 epoch 22 - iter 5/19 - loss 0.89454805 - samples/sec: 495.04 - lr: 0.012500\n",
            "2022-03-03 15:56:34,088 epoch 22 - iter 6/19 - loss 0.90607178 - samples/sec: 502.58 - lr: 0.012500\n",
            "2022-03-03 15:56:34,157 epoch 22 - iter 7/19 - loss 0.92409909 - samples/sec: 522.58 - lr: 0.012500\n",
            "2022-03-03 15:56:34,232 epoch 22 - iter 8/19 - loss 0.91293430 - samples/sec: 506.40 - lr: 0.012500\n",
            "2022-03-03 15:56:34,304 epoch 22 - iter 9/19 - loss 0.91247289 - samples/sec: 456.16 - lr: 0.012500\n",
            "2022-03-03 15:56:34,369 epoch 22 - iter 10/19 - loss 0.90747681 - samples/sec: 531.11 - lr: 0.012500\n",
            "2022-03-03 15:56:34,444 epoch 22 - iter 11/19 - loss 0.89900234 - samples/sec: 539.92 - lr: 0.012500\n",
            "2022-03-03 15:56:34,515 epoch 22 - iter 12/19 - loss 0.90169982 - samples/sec: 550.00 - lr: 0.012500\n",
            "2022-03-03 15:56:34,577 epoch 22 - iter 13/19 - loss 0.91120514 - samples/sec: 524.30 - lr: 0.012500\n",
            "2022-03-03 15:56:34,650 epoch 22 - iter 14/19 - loss 0.90737333 - samples/sec: 528.02 - lr: 0.012500\n",
            "2022-03-03 15:56:34,715 epoch 22 - iter 15/19 - loss 0.90301132 - samples/sec: 555.57 - lr: 0.012500\n",
            "2022-03-03 15:56:34,787 epoch 22 - iter 16/19 - loss 0.89905499 - samples/sec: 490.24 - lr: 0.012500\n",
            "2022-03-03 15:56:34,859 epoch 22 - iter 17/19 - loss 0.89339888 - samples/sec: 557.70 - lr: 0.012500\n",
            "2022-03-03 15:56:34,924 epoch 22 - iter 18/19 - loss 0.89427290 - samples/sec: 605.06 - lr: 0.012500\n",
            "2022-03-03 15:56:34,979 epoch 22 - iter 19/19 - loss 0.89286927 - samples/sec: 625.89 - lr: 0.012500\n",
            "2022-03-03 15:56:35,437 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:35,440 EPOCH 22 done: loss 0.8929 - lr 0.0125000\n",
            "2022-03-03 15:56:36,753 DEV : loss 1.2183997631072998 - score 0.3788\n",
            "2022-03-03 15:56:36,824 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 15:56:36,826 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:37,770 epoch 23 - iter 1/19 - loss 0.86258888 - samples/sec: 200.45 - lr: 0.012500\n",
            "2022-03-03 15:56:37,854 epoch 23 - iter 2/19 - loss 0.95029461 - samples/sec: 452.18 - lr: 0.012500\n",
            "2022-03-03 15:56:37,937 epoch 23 - iter 3/19 - loss 0.88421778 - samples/sec: 468.39 - lr: 0.012500\n",
            "2022-03-03 15:56:38,018 epoch 23 - iter 4/19 - loss 0.88942906 - samples/sec: 475.20 - lr: 0.012500\n",
            "2022-03-03 15:56:38,100 epoch 23 - iter 5/19 - loss 0.89389937 - samples/sec: 457.46 - lr: 0.012500\n",
            "2022-03-03 15:56:38,221 epoch 23 - iter 6/19 - loss 0.86045354 - samples/sec: 465.64 - lr: 0.012500\n",
            "2022-03-03 15:56:38,302 epoch 23 - iter 7/19 - loss 0.87183664 - samples/sec: 428.93 - lr: 0.012500\n",
            "2022-03-03 15:56:38,373 epoch 23 - iter 8/19 - loss 0.86235363 - samples/sec: 470.61 - lr: 0.012500\n",
            "2022-03-03 15:56:38,443 epoch 23 - iter 9/19 - loss 0.86825783 - samples/sec: 470.80 - lr: 0.012500\n",
            "2022-03-03 15:56:38,514 epoch 23 - iter 10/19 - loss 0.87044925 - samples/sec: 505.27 - lr: 0.012500\n",
            "2022-03-03 15:56:38,577 epoch 23 - iter 11/19 - loss 0.88432468 - samples/sec: 519.64 - lr: 0.012500\n",
            "2022-03-03 15:56:38,648 epoch 23 - iter 12/19 - loss 0.87469178 - samples/sec: 546.53 - lr: 0.012500\n",
            "2022-03-03 15:56:38,719 epoch 23 - iter 13/19 - loss 0.87622228 - samples/sec: 462.45 - lr: 0.012500\n",
            "2022-03-03 15:56:38,788 epoch 23 - iter 14/19 - loss 0.88130353 - samples/sec: 480.25 - lr: 0.012500\n",
            "2022-03-03 15:56:38,858 epoch 23 - iter 15/19 - loss 0.88789242 - samples/sec: 464.87 - lr: 0.012500\n",
            "2022-03-03 15:56:38,927 epoch 23 - iter 16/19 - loss 0.88596714 - samples/sec: 480.49 - lr: 0.012500\n",
            "2022-03-03 15:56:38,998 epoch 23 - iter 17/19 - loss 0.89170547 - samples/sec: 464.30 - lr: 0.012500\n",
            "2022-03-03 15:56:39,074 epoch 23 - iter 18/19 - loss 0.88669534 - samples/sec: 520.08 - lr: 0.012500\n",
            "2022-03-03 15:56:39,134 epoch 23 - iter 19/19 - loss 0.88524088 - samples/sec: 595.72 - lr: 0.012500\n",
            "2022-03-03 15:56:39,622 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:39,624 EPOCH 23 done: loss 0.8852 - lr 0.0125000\n",
            "2022-03-03 15:56:40,943 DEV : loss 1.2187011241912842 - score 0.3788\n",
            "2022-03-03 15:56:41,016 BAD EPOCHS (no improvement): 2\n",
            "2022-03-03 15:56:41,018 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:41,946 epoch 24 - iter 1/19 - loss 0.82001507 - samples/sec: 164.41 - lr: 0.012500\n",
            "2022-03-03 15:56:42,030 epoch 24 - iter 2/19 - loss 0.91145581 - samples/sec: 460.90 - lr: 0.012500\n",
            "2022-03-03 15:56:42,127 epoch 24 - iter 3/19 - loss 0.83423859 - samples/sec: 464.32 - lr: 0.012500\n",
            "2022-03-03 15:56:42,217 epoch 24 - iter 4/19 - loss 0.82877962 - samples/sec: 459.90 - lr: 0.012500\n",
            "2022-03-03 15:56:42,287 epoch 24 - iter 5/19 - loss 0.87420176 - samples/sec: 471.28 - lr: 0.012500\n",
            "2022-03-03 15:56:42,376 epoch 24 - iter 6/19 - loss 0.88786762 - samples/sec: 401.16 - lr: 0.012500\n",
            "2022-03-03 15:56:42,450 epoch 24 - iter 7/19 - loss 0.88420182 - samples/sec: 499.96 - lr: 0.012500\n",
            "2022-03-03 15:56:42,532 epoch 24 - iter 8/19 - loss 0.87743697 - samples/sec: 541.26 - lr: 0.012500\n",
            "2022-03-03 15:56:42,599 epoch 24 - iter 9/19 - loss 0.88326879 - samples/sec: 488.06 - lr: 0.012500\n",
            "2022-03-03 15:56:42,673 epoch 24 - iter 10/19 - loss 0.89257264 - samples/sec: 552.88 - lr: 0.012500\n",
            "2022-03-03 15:56:42,738 epoch 24 - iter 11/19 - loss 0.89524298 - samples/sec: 504.32 - lr: 0.012500\n",
            "2022-03-03 15:56:42,806 epoch 24 - iter 12/19 - loss 0.89922172 - samples/sec: 486.51 - lr: 0.012500\n",
            "2022-03-03 15:56:42,875 epoch 24 - iter 13/19 - loss 0.89869443 - samples/sec: 473.12 - lr: 0.012500\n",
            "2022-03-03 15:56:42,956 epoch 24 - iter 14/19 - loss 0.89936970 - samples/sec: 543.77 - lr: 0.012500\n",
            "2022-03-03 15:56:43,031 epoch 24 - iter 15/19 - loss 0.89992970 - samples/sec: 441.10 - lr: 0.012500\n",
            "2022-03-03 15:56:43,103 epoch 24 - iter 16/19 - loss 0.89655214 - samples/sec: 497.82 - lr: 0.012500\n",
            "2022-03-03 15:56:43,177 epoch 24 - iter 17/19 - loss 0.89722085 - samples/sec: 465.54 - lr: 0.012500\n",
            "2022-03-03 15:56:43,240 epoch 24 - iter 18/19 - loss 0.89099033 - samples/sec: 519.00 - lr: 0.012500\n",
            "2022-03-03 15:56:43,291 epoch 24 - iter 19/19 - loss 0.88922059 - samples/sec: 656.59 - lr: 0.012500\n",
            "2022-03-03 15:56:43,753 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:43,756 EPOCH 24 done: loss 0.8892 - lr 0.0125000\n",
            "2022-03-03 15:56:45,072 DEV : loss 1.214099407196045 - score 0.3788\n",
            "2022-03-03 15:56:45,149 BAD EPOCHS (no improvement): 3\n",
            "2022-03-03 15:56:45,157 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:46,094 epoch 25 - iter 1/19 - loss 0.88829696 - samples/sec: 157.44 - lr: 0.012500\n",
            "2022-03-03 15:56:46,181 epoch 25 - iter 2/19 - loss 0.91575336 - samples/sec: 470.43 - lr: 0.012500\n",
            "2022-03-03 15:56:46,264 epoch 25 - iter 3/19 - loss 0.94343791 - samples/sec: 457.72 - lr: 0.012500\n",
            "2022-03-03 15:56:46,349 epoch 25 - iter 4/19 - loss 0.93544449 - samples/sec: 484.57 - lr: 0.012500\n",
            "2022-03-03 15:56:46,424 epoch 25 - iter 5/19 - loss 0.92795379 - samples/sec: 445.62 - lr: 0.012500\n",
            "2022-03-03 15:56:46,493 epoch 25 - iter 6/19 - loss 0.93827391 - samples/sec: 565.21 - lr: 0.012500\n",
            "2022-03-03 15:56:46,565 epoch 25 - iter 7/19 - loss 0.91662822 - samples/sec: 496.16 - lr: 0.012500\n",
            "2022-03-03 15:56:46,645 epoch 25 - iter 8/19 - loss 0.89839707 - samples/sec: 508.73 - lr: 0.012500\n",
            "2022-03-03 15:56:46,714 epoch 25 - iter 9/19 - loss 0.88775724 - samples/sec: 556.30 - lr: 0.012500\n",
            "2022-03-03 15:56:46,791 epoch 25 - iter 10/19 - loss 0.88229135 - samples/sec: 557.68 - lr: 0.012500\n",
            "2022-03-03 15:56:46,863 epoch 25 - iter 11/19 - loss 0.88248897 - samples/sec: 454.35 - lr: 0.012500\n",
            "2022-03-03 15:56:46,928 epoch 25 - iter 12/19 - loss 0.89266361 - samples/sec: 599.98 - lr: 0.012500\n",
            "2022-03-03 15:56:47,001 epoch 25 - iter 13/19 - loss 0.89873657 - samples/sec: 450.74 - lr: 0.012500\n",
            "2022-03-03 15:56:47,072 epoch 25 - iter 14/19 - loss 0.90300514 - samples/sec: 576.59 - lr: 0.012500\n",
            "2022-03-03 15:56:47,143 epoch 25 - iter 15/19 - loss 0.90170612 - samples/sec: 499.35 - lr: 0.012500\n",
            "2022-03-03 15:56:47,220 epoch 25 - iter 16/19 - loss 0.89398136 - samples/sec: 518.43 - lr: 0.012500\n",
            "2022-03-03 15:56:47,288 epoch 25 - iter 17/19 - loss 0.88900539 - samples/sec: 485.75 - lr: 0.012500\n",
            "2022-03-03 15:56:47,362 epoch 25 - iter 18/19 - loss 0.88772959 - samples/sec: 514.59 - lr: 0.012500\n",
            "2022-03-03 15:56:47,414 epoch 25 - iter 19/19 - loss 0.87758751 - samples/sec: 639.69 - lr: 0.012500\n",
            "2022-03-03 15:56:47,846 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:47,848 EPOCH 25 done: loss 0.8776 - lr 0.0125000\n",
            "2022-03-03 15:56:49,182 DEV : loss 1.2191228866577148 - score 0.3788\n",
            "Epoch    25: reducing learning rate of group 0 to 6.2500e-03.\n",
            "2022-03-03 15:56:49,259 BAD EPOCHS (no improvement): 4\n",
            "2022-03-03 15:56:49,262 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:50,217 epoch 26 - iter 1/19 - loss 0.64743942 - samples/sec: 201.00 - lr: 0.006250\n",
            "2022-03-03 15:56:50,300 epoch 26 - iter 2/19 - loss 0.80782962 - samples/sec: 454.11 - lr: 0.006250\n",
            "2022-03-03 15:56:50,385 epoch 26 - iter 3/19 - loss 0.80904849 - samples/sec: 436.48 - lr: 0.006250\n",
            "2022-03-03 15:56:50,464 epoch 26 - iter 4/19 - loss 0.85086490 - samples/sec: 508.26 - lr: 0.006250\n",
            "2022-03-03 15:56:50,556 epoch 26 - iter 5/19 - loss 0.86947671 - samples/sec: 462.47 - lr: 0.006250\n",
            "2022-03-03 15:56:50,633 epoch 26 - iter 6/19 - loss 0.86322659 - samples/sec: 468.75 - lr: 0.006250\n",
            "2022-03-03 15:56:50,707 epoch 26 - iter 7/19 - loss 0.89300956 - samples/sec: 522.20 - lr: 0.006250\n",
            "2022-03-03 15:56:50,778 epoch 26 - iter 8/19 - loss 0.89019639 - samples/sec: 464.36 - lr: 0.006250\n",
            "2022-03-03 15:56:50,879 epoch 26 - iter 9/19 - loss 0.88932972 - samples/sec: 584.47 - lr: 0.006250\n",
            "2022-03-03 15:56:50,946 epoch 26 - iter 10/19 - loss 0.88064857 - samples/sec: 486.94 - lr: 0.006250\n",
            "2022-03-03 15:56:51,009 epoch 26 - iter 11/19 - loss 0.88098941 - samples/sec: 521.97 - lr: 0.006250\n",
            "2022-03-03 15:56:51,078 epoch 26 - iter 12/19 - loss 0.88681289 - samples/sec: 479.59 - lr: 0.006250\n",
            "2022-03-03 15:56:51,149 epoch 26 - iter 13/19 - loss 0.88381172 - samples/sec: 460.62 - lr: 0.006250\n",
            "2022-03-03 15:56:51,224 epoch 26 - iter 14/19 - loss 0.88372106 - samples/sec: 564.12 - lr: 0.006250\n",
            "2022-03-03 15:56:51,299 epoch 26 - iter 15/19 - loss 0.88788667 - samples/sec: 504.82 - lr: 0.006250\n",
            "2022-03-03 15:56:51,366 epoch 26 - iter 16/19 - loss 0.88171589 - samples/sec: 490.71 - lr: 0.006250\n",
            "2022-03-03 15:56:51,437 epoch 26 - iter 17/19 - loss 0.87974966 - samples/sec: 504.58 - lr: 0.006250\n",
            "2022-03-03 15:56:51,506 epoch 26 - iter 18/19 - loss 0.87705291 - samples/sec: 476.04 - lr: 0.006250\n",
            "2022-03-03 15:56:51,556 epoch 26 - iter 19/19 - loss 0.87879210 - samples/sec: 666.10 - lr: 0.006250\n",
            "2022-03-03 15:56:52,021 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:52,023 EPOCH 26 done: loss 0.8788 - lr 0.0062500\n",
            "2022-03-03 15:56:53,322 DEV : loss 1.218406081199646 - score 0.3788\n",
            "2022-03-03 15:56:53,395 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 15:56:53,398 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:54,349 epoch 27 - iter 1/19 - loss 0.77669156 - samples/sec: 154.78 - lr: 0.006250\n",
            "2022-03-03 15:56:54,438 epoch 27 - iter 2/19 - loss 0.77285251 - samples/sec: 482.55 - lr: 0.006250\n",
            "2022-03-03 15:56:54,522 epoch 27 - iter 3/19 - loss 0.80334183 - samples/sec: 472.96 - lr: 0.006250\n",
            "2022-03-03 15:56:54,609 epoch 27 - iter 4/19 - loss 0.80462779 - samples/sec: 446.94 - lr: 0.006250\n",
            "2022-03-03 15:56:54,681 epoch 27 - iter 5/19 - loss 0.82460071 - samples/sec: 497.80 - lr: 0.006250\n",
            "2022-03-03 15:56:54,756 epoch 27 - iter 6/19 - loss 0.84055089 - samples/sec: 451.51 - lr: 0.006250\n",
            "2022-03-03 15:56:54,832 epoch 27 - iter 7/19 - loss 0.84093979 - samples/sec: 479.73 - lr: 0.006250\n",
            "2022-03-03 15:56:54,907 epoch 27 - iter 8/19 - loss 0.85554918 - samples/sec: 566.29 - lr: 0.006250\n",
            "2022-03-03 15:56:55,012 epoch 27 - iter 9/19 - loss 0.86586513 - samples/sec: 512.01 - lr: 0.006250\n",
            "2022-03-03 15:56:55,094 epoch 27 - iter 10/19 - loss 0.87369081 - samples/sec: 518.55 - lr: 0.006250\n",
            "2022-03-03 15:56:55,159 epoch 27 - iter 11/19 - loss 0.87945299 - samples/sec: 530.39 - lr: 0.006250\n",
            "2022-03-03 15:56:55,229 epoch 27 - iter 12/19 - loss 0.88805279 - samples/sec: 469.90 - lr: 0.006250\n",
            "2022-03-03 15:56:55,298 epoch 27 - iter 13/19 - loss 0.89781050 - samples/sec: 481.50 - lr: 0.006250\n",
            "2022-03-03 15:56:55,362 epoch 27 - iter 14/19 - loss 0.89791439 - samples/sec: 509.87 - lr: 0.006250\n",
            "2022-03-03 15:56:55,431 epoch 27 - iter 15/19 - loss 0.88845836 - samples/sec: 529.62 - lr: 0.006250\n",
            "2022-03-03 15:56:55,504 epoch 27 - iter 16/19 - loss 0.89324076 - samples/sec: 486.01 - lr: 0.006250\n",
            "2022-03-03 15:56:55,574 epoch 27 - iter 17/19 - loss 0.88641126 - samples/sec: 467.28 - lr: 0.006250\n",
            "2022-03-03 15:56:55,642 epoch 27 - iter 18/19 - loss 0.88651600 - samples/sec: 490.15 - lr: 0.006250\n",
            "2022-03-03 15:56:55,692 epoch 27 - iter 19/19 - loss 0.87876652 - samples/sec: 666.76 - lr: 0.006250\n",
            "2022-03-03 15:56:56,139 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:56:56,141 EPOCH 27 done: loss 0.8788 - lr 0.0062500\n",
            "2022-03-03 15:56:57,486 DEV : loss 1.2156246900558472 - score 0.3788\n",
            "2022-03-03 15:56:57,561 BAD EPOCHS (no improvement): 2\n",
            "2022-03-03 15:56:57,563 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:56:58,531 epoch 28 - iter 1/19 - loss 0.88586396 - samples/sec: 172.74 - lr: 0.006250\n",
            "2022-03-03 15:56:58,613 epoch 28 - iter 2/19 - loss 0.83273861 - samples/sec: 472.30 - lr: 0.006250\n",
            "2022-03-03 15:56:58,710 epoch 28 - iter 3/19 - loss 0.81615867 - samples/sec: 431.43 - lr: 0.006250\n",
            "2022-03-03 15:56:58,789 epoch 28 - iter 4/19 - loss 0.80454075 - samples/sec: 465.93 - lr: 0.006250\n",
            "2022-03-03 15:56:58,861 epoch 28 - iter 5/19 - loss 0.81612303 - samples/sec: 461.86 - lr: 0.006250\n",
            "2022-03-03 15:56:58,949 epoch 28 - iter 6/19 - loss 0.83436000 - samples/sec: 458.98 - lr: 0.006250\n",
            "2022-03-03 15:56:59,016 epoch 28 - iter 7/19 - loss 0.83784779 - samples/sec: 488.47 - lr: 0.006250\n",
            "2022-03-03 15:56:59,090 epoch 28 - iter 8/19 - loss 0.85158770 - samples/sec: 488.46 - lr: 0.006250\n",
            "2022-03-03 15:56:59,174 epoch 28 - iter 9/19 - loss 0.86471419 - samples/sec: 533.13 - lr: 0.006250\n",
            "2022-03-03 15:56:59,247 epoch 28 - iter 10/19 - loss 0.85735970 - samples/sec: 454.09 - lr: 0.006250\n",
            "2022-03-03 15:56:59,314 epoch 28 - iter 11/19 - loss 0.86552413 - samples/sec: 489.48 - lr: 0.006250\n",
            "2022-03-03 15:56:59,392 epoch 28 - iter 12/19 - loss 0.87319794 - samples/sec: 513.42 - lr: 0.006250\n",
            "2022-03-03 15:56:59,464 epoch 28 - iter 13/19 - loss 0.86971486 - samples/sec: 455.90 - lr: 0.006250\n",
            "2022-03-03 15:56:59,531 epoch 28 - iter 14/19 - loss 0.86670876 - samples/sec: 529.67 - lr: 0.006250\n",
            "2022-03-03 15:56:59,610 epoch 28 - iter 15/19 - loss 0.86883613 - samples/sec: 531.66 - lr: 0.006250\n",
            "2022-03-03 15:56:59,674 epoch 28 - iter 16/19 - loss 0.87722637 - samples/sec: 515.91 - lr: 0.006250\n",
            "2022-03-03 15:56:59,740 epoch 28 - iter 17/19 - loss 0.88220264 - samples/sec: 495.18 - lr: 0.006250\n",
            "2022-03-03 15:56:59,812 epoch 28 - iter 18/19 - loss 0.88207612 - samples/sec: 548.77 - lr: 0.006250\n",
            "2022-03-03 15:56:59,860 epoch 28 - iter 19/19 - loss 0.87941760 - samples/sec: 694.68 - lr: 0.006250\n",
            "2022-03-03 15:57:00,292 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:57:00,294 EPOCH 28 done: loss 0.8794 - lr 0.0062500\n",
            "2022-03-03 15:57:01,604 DEV : loss 1.2144984006881714 - score 0.3788\n",
            "2022-03-03 15:57:01,676 BAD EPOCHS (no improvement): 3\n",
            "2022-03-03 15:57:01,678 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:57:02,624 epoch 29 - iter 1/19 - loss 0.82917804 - samples/sec: 213.92 - lr: 0.006250\n",
            "2022-03-03 15:57:05,645 epoch 29 - iter 2/19 - loss 0.85267147 - samples/sec: 10.61 - lr: 0.006250\n",
            "2022-03-03 15:57:05,740 epoch 29 - iter 3/19 - loss 0.85496602 - samples/sec: 370.44 - lr: 0.006250\n",
            "2022-03-03 15:57:05,826 epoch 29 - iter 4/19 - loss 0.88419773 - samples/sec: 390.80 - lr: 0.006250\n",
            "2022-03-03 15:57:05,989 epoch 29 - iter 5/19 - loss 0.83626070 - samples/sec: 260.88 - lr: 0.006250\n",
            "2022-03-03 15:57:09,195 epoch 29 - iter 6/19 - loss 0.83211268 - samples/sec: 464.16 - lr: 0.006250\n",
            "2022-03-03 15:57:09,295 epoch 29 - iter 7/19 - loss 0.84130251 - samples/sec: 361.32 - lr: 0.006250\n",
            "2022-03-03 15:57:09,378 epoch 29 - iter 8/19 - loss 0.86295562 - samples/sec: 439.62 - lr: 0.006250\n",
            "2022-03-03 15:57:09,465 epoch 29 - iter 9/19 - loss 0.87567571 - samples/sec: 417.22 - lr: 0.006250\n",
            "2022-03-03 15:57:09,563 epoch 29 - iter 10/19 - loss 0.88276651 - samples/sec: 359.11 - lr: 0.006250\n",
            "2022-03-03 15:57:09,643 epoch 29 - iter 11/19 - loss 0.88198260 - samples/sec: 432.99 - lr: 0.006250\n",
            "2022-03-03 15:57:10,461 epoch 29 - iter 12/19 - loss 0.87548670 - samples/sec: 548.84 - lr: 0.006250\n",
            "2022-03-03 15:57:10,542 epoch 29 - iter 13/19 - loss 0.87740643 - samples/sec: 407.29 - lr: 0.006250\n",
            "2022-03-03 15:57:10,600 epoch 29 - iter 14/19 - loss 0.87121946 - samples/sec: 564.65 - lr: 0.006250\n",
            "2022-03-03 15:57:10,670 epoch 29 - iter 15/19 - loss 0.87688461 - samples/sec: 473.43 - lr: 0.006250\n",
            "2022-03-03 15:57:10,735 epoch 29 - iter 16/19 - loss 0.89036446 - samples/sec: 502.84 - lr: 0.006250\n",
            "2022-03-03 15:57:10,800 epoch 29 - iter 17/19 - loss 0.89197852 - samples/sec: 557.71 - lr: 0.006250\n",
            "2022-03-03 15:57:10,870 epoch 29 - iter 18/19 - loss 0.88705535 - samples/sec: 543.18 - lr: 0.006250\n",
            "2022-03-03 15:57:10,922 epoch 29 - iter 19/19 - loss 0.88339811 - samples/sec: 657.46 - lr: 0.006250\n",
            "2022-03-03 15:57:11,419 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:57:11,421 EPOCH 29 done: loss 0.8834 - lr 0.0062500\n",
            "2022-03-03 15:57:12,663 DEV : loss 1.2133476734161377 - score 0.3788\n",
            "Epoch    29: reducing learning rate of group 0 to 3.1250e-03.\n",
            "2022-03-03 15:57:12,748 BAD EPOCHS (no improvement): 4\n",
            "2022-03-03 15:57:12,752 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:57:13,642 epoch 30 - iter 1/19 - loss 0.88628697 - samples/sec: 181.58 - lr: 0.003125\n",
            "2022-03-03 15:57:13,750 epoch 30 - iter 2/19 - loss 0.88551831 - samples/sec: 397.39 - lr: 0.003125\n",
            "2022-03-03 15:57:13,839 epoch 30 - iter 3/19 - loss 0.88614553 - samples/sec: 419.94 - lr: 0.003125\n",
            "2022-03-03 15:57:13,910 epoch 30 - iter 4/19 - loss 0.87454471 - samples/sec: 479.61 - lr: 0.003125\n",
            "2022-03-03 15:57:13,985 epoch 30 - iter 5/19 - loss 0.88594788 - samples/sec: 504.60 - lr: 0.003125\n",
            "2022-03-03 15:57:14,069 epoch 30 - iter 6/19 - loss 0.88107973 - samples/sec: 477.01 - lr: 0.003125\n",
            "2022-03-03 15:57:14,133 epoch 30 - iter 7/19 - loss 0.90813872 - samples/sec: 517.73 - lr: 0.003125\n",
            "2022-03-03 15:57:14,216 epoch 30 - iter 8/19 - loss 0.91420961 - samples/sec: 531.17 - lr: 0.003125\n",
            "2022-03-03 15:57:14,292 epoch 30 - iter 9/19 - loss 0.89169084 - samples/sec: 476.05 - lr: 0.003125\n",
            "2022-03-03 15:57:14,361 epoch 30 - iter 10/19 - loss 0.88626651 - samples/sec: 481.11 - lr: 0.003125\n",
            "2022-03-03 15:57:14,434 epoch 30 - iter 11/19 - loss 0.89315398 - samples/sec: 450.59 - lr: 0.003125\n",
            "2022-03-03 15:57:14,506 epoch 30 - iter 12/19 - loss 0.88259125 - samples/sec: 581.71 - lr: 0.003125\n",
            "2022-03-03 15:57:14,570 epoch 30 - iter 13/19 - loss 0.88516932 - samples/sec: 509.75 - lr: 0.003125\n",
            "2022-03-03 15:57:14,647 epoch 30 - iter 14/19 - loss 0.88365232 - samples/sec: 563.19 - lr: 0.003125\n",
            "2022-03-03 15:57:14,713 epoch 30 - iter 15/19 - loss 0.88128042 - samples/sec: 540.89 - lr: 0.003125\n",
            "2022-03-03 15:57:14,796 epoch 30 - iter 16/19 - loss 0.88032998 - samples/sec: 396.50 - lr: 0.003125\n",
            "2022-03-03 15:57:14,863 epoch 30 - iter 17/19 - loss 0.87945029 - samples/sec: 499.34 - lr: 0.003125\n",
            "2022-03-03 15:57:14,933 epoch 30 - iter 18/19 - loss 0.88339037 - samples/sec: 580.78 - lr: 0.003125\n",
            "2022-03-03 15:57:14,990 epoch 30 - iter 19/19 - loss 0.87422713 - samples/sec: 617.39 - lr: 0.003125\n",
            "2022-03-03 15:57:15,387 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:57:15,393 EPOCH 30 done: loss 0.8742 - lr 0.0031250\n",
            "2022-03-03 15:57:16,626 DEV : loss 1.2136790752410889 - score 0.3788\n",
            "2022-03-03 15:57:16,706 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 15:57:16,709 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:57:17,643 epoch 31 - iter 1/19 - loss 0.94522107 - samples/sec: 226.64 - lr: 0.003125\n",
            "2022-03-03 15:57:17,729 epoch 31 - iter 2/19 - loss 0.91141063 - samples/sec: 434.63 - lr: 0.003125\n",
            "2022-03-03 15:57:17,819 epoch 31 - iter 3/19 - loss 0.90921497 - samples/sec: 435.14 - lr: 0.003125\n",
            "2022-03-03 15:57:17,895 epoch 31 - iter 4/19 - loss 0.89234285 - samples/sec: 471.69 - lr: 0.003125\n",
            "2022-03-03 15:57:17,970 epoch 31 - iter 5/19 - loss 0.89022639 - samples/sec: 494.62 - lr: 0.003125\n",
            "2022-03-03 15:57:18,055 epoch 31 - iter 6/19 - loss 0.90197215 - samples/sec: 475.96 - lr: 0.003125\n",
            "2022-03-03 15:57:18,142 epoch 31 - iter 7/19 - loss 0.90247300 - samples/sec: 417.86 - lr: 0.003125\n",
            "2022-03-03 15:57:18,210 epoch 31 - iter 8/19 - loss 0.91170024 - samples/sec: 494.48 - lr: 0.003125\n",
            "2022-03-03 15:57:18,278 epoch 31 - iter 9/19 - loss 0.91895789 - samples/sec: 605.69 - lr: 0.003125\n",
            "2022-03-03 15:57:18,358 epoch 31 - iter 10/19 - loss 0.90890926 - samples/sec: 486.01 - lr: 0.003125\n",
            "2022-03-03 15:57:18,426 epoch 31 - iter 11/19 - loss 0.90793572 - samples/sec: 485.01 - lr: 0.003125\n",
            "2022-03-03 15:57:18,503 epoch 31 - iter 12/19 - loss 0.88729584 - samples/sec: 549.73 - lr: 0.003125\n",
            "2022-03-03 15:57:18,586 epoch 31 - iter 13/19 - loss 0.88576284 - samples/sec: 533.97 - lr: 0.003125\n",
            "2022-03-03 15:57:18,654 epoch 31 - iter 14/19 - loss 0.89046803 - samples/sec: 528.60 - lr: 0.003125\n",
            "2022-03-03 15:57:18,723 epoch 31 - iter 15/19 - loss 0.89688771 - samples/sec: 518.90 - lr: 0.003125\n",
            "2022-03-03 15:57:18,787 epoch 31 - iter 16/19 - loss 0.89318680 - samples/sec: 521.31 - lr: 0.003125\n",
            "2022-03-03 15:57:18,854 epoch 31 - iter 17/19 - loss 0.89106616 - samples/sec: 496.42 - lr: 0.003125\n",
            "2022-03-03 15:57:18,929 epoch 31 - iter 18/19 - loss 0.88477417 - samples/sec: 435.22 - lr: 0.003125\n",
            "2022-03-03 15:57:18,989 epoch 31 - iter 19/19 - loss 0.87339414 - samples/sec: 651.72 - lr: 0.003125\n",
            "2022-03-03 15:57:19,394 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:57:19,398 EPOCH 31 done: loss 0.8734 - lr 0.0031250\n",
            "2022-03-03 15:57:20,615 DEV : loss 1.2137693166732788 - score 0.3788\n",
            "2022-03-03 15:57:20,689 BAD EPOCHS (no improvement): 2\n",
            "2022-03-03 15:57:20,696 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:57:21,588 epoch 32 - iter 1/19 - loss 0.92055148 - samples/sec: 191.08 - lr: 0.003125\n",
            "2022-03-03 15:57:21,680 epoch 32 - iter 2/19 - loss 0.88899985 - samples/sec: 418.09 - lr: 0.003125\n",
            "2022-03-03 15:57:21,760 epoch 32 - iter 3/19 - loss 0.88719014 - samples/sec: 483.69 - lr: 0.003125\n",
            "2022-03-03 15:57:21,853 epoch 32 - iter 4/19 - loss 0.88033667 - samples/sec: 482.08 - lr: 0.003125\n",
            "2022-03-03 15:57:21,931 epoch 32 - iter 5/19 - loss 0.88738528 - samples/sec: 419.78 - lr: 0.003125\n",
            "2022-03-03 15:57:22,008 epoch 32 - iter 6/19 - loss 0.89826841 - samples/sec: 485.95 - lr: 0.003125\n",
            "2022-03-03 15:57:22,081 epoch 32 - iter 7/19 - loss 0.88858103 - samples/sec: 458.58 - lr: 0.003125\n",
            "2022-03-03 15:57:22,148 epoch 32 - iter 8/19 - loss 0.87517010 - samples/sec: 494.79 - lr: 0.003125\n",
            "2022-03-03 15:57:22,240 epoch 32 - iter 9/19 - loss 0.86921917 - samples/sec: 554.61 - lr: 0.003125\n",
            "2022-03-03 15:57:22,325 epoch 32 - iter 10/19 - loss 0.87990224 - samples/sec: 482.20 - lr: 0.003125\n",
            "2022-03-03 15:57:22,389 epoch 32 - iter 11/19 - loss 0.87644614 - samples/sec: 515.49 - lr: 0.003125\n",
            "2022-03-03 15:57:22,457 epoch 32 - iter 12/19 - loss 0.87351348 - samples/sec: 483.27 - lr: 0.003125\n",
            "2022-03-03 15:57:22,525 epoch 32 - iter 13/19 - loss 0.87276644 - samples/sec: 518.66 - lr: 0.003125\n",
            "2022-03-03 15:57:22,590 epoch 32 - iter 14/19 - loss 0.86903772 - samples/sec: 515.75 - lr: 0.003125\n",
            "2022-03-03 15:57:22,658 epoch 32 - iter 15/19 - loss 0.87550891 - samples/sec: 486.00 - lr: 0.003125\n",
            "2022-03-03 15:57:22,733 epoch 32 - iter 16/19 - loss 0.87626259 - samples/sec: 529.29 - lr: 0.003125\n",
            "2022-03-03 15:57:22,810 epoch 32 - iter 17/19 - loss 0.87159351 - samples/sec: 427.79 - lr: 0.003125\n",
            "2022-03-03 15:57:22,875 epoch 32 - iter 18/19 - loss 0.87540977 - samples/sec: 530.52 - lr: 0.003125\n",
            "2022-03-03 15:57:22,923 epoch 32 - iter 19/19 - loss 0.87654159 - samples/sec: 693.53 - lr: 0.003125\n",
            "2022-03-03 15:57:23,337 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:57:23,341 EPOCH 32 done: loss 0.8765 - lr 0.0031250\n",
            "2022-03-03 15:57:24,573 DEV : loss 1.214325189590454 - score 0.3788\n",
            "2022-03-03 15:57:24,651 BAD EPOCHS (no improvement): 3\n",
            "2022-03-03 15:57:24,654 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:57:25,560 epoch 33 - iter 1/19 - loss 0.83431709 - samples/sec: 156.90 - lr: 0.003125\n",
            "2022-03-03 15:57:25,648 epoch 33 - iter 2/19 - loss 0.84577370 - samples/sec: 472.11 - lr: 0.003125\n",
            "2022-03-03 15:57:25,734 epoch 33 - iter 3/19 - loss 0.85121445 - samples/sec: 470.22 - lr: 0.003125\n",
            "2022-03-03 15:57:25,809 epoch 33 - iter 4/19 - loss 0.87730579 - samples/sec: 512.48 - lr: 0.003125\n",
            "2022-03-03 15:57:25,875 epoch 33 - iter 5/19 - loss 0.85831405 - samples/sec: 502.82 - lr: 0.003125\n",
            "2022-03-03 15:57:25,961 epoch 33 - iter 6/19 - loss 0.85100632 - samples/sec: 477.72 - lr: 0.003125\n",
            "2022-03-03 15:57:26,037 epoch 33 - iter 7/19 - loss 0.85199405 - samples/sec: 434.19 - lr: 0.003125\n",
            "2022-03-03 15:57:26,114 epoch 33 - iter 8/19 - loss 0.85210013 - samples/sec: 483.86 - lr: 0.003125\n",
            "2022-03-03 15:57:26,191 epoch 33 - iter 9/19 - loss 0.86478240 - samples/sec: 515.90 - lr: 0.003125\n",
            "2022-03-03 15:57:26,263 epoch 33 - iter 10/19 - loss 0.86505882 - samples/sec: 511.95 - lr: 0.003125\n",
            "2022-03-03 15:57:26,331 epoch 33 - iter 11/19 - loss 0.87263064 - samples/sec: 483.87 - lr: 0.003125\n",
            "2022-03-03 15:57:26,409 epoch 33 - iter 12/19 - loss 0.87587248 - samples/sec: 543.60 - lr: 0.003125\n",
            "2022-03-03 15:57:26,483 epoch 33 - iter 13/19 - loss 0.88361550 - samples/sec: 457.34 - lr: 0.003125\n",
            "2022-03-03 15:57:26,558 epoch 33 - iter 14/19 - loss 0.88760875 - samples/sec: 482.40 - lr: 0.003125\n",
            "2022-03-03 15:57:26,631 epoch 33 - iter 15/19 - loss 0.88023111 - samples/sec: 542.40 - lr: 0.003125\n",
            "2022-03-03 15:57:26,701 epoch 33 - iter 16/19 - loss 0.87963621 - samples/sec: 517.37 - lr: 0.003125\n",
            "2022-03-03 15:57:26,770 epoch 33 - iter 17/19 - loss 0.87093882 - samples/sec: 479.81 - lr: 0.003125\n",
            "2022-03-03 15:57:26,839 epoch 33 - iter 18/19 - loss 0.87759557 - samples/sec: 527.66 - lr: 0.003125\n",
            "2022-03-03 15:57:26,898 epoch 33 - iter 19/19 - loss 0.87329768 - samples/sec: 552.56 - lr: 0.003125\n",
            "2022-03-03 15:57:27,297 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:57:27,305 EPOCH 33 done: loss 0.8733 - lr 0.0031250\n",
            "2022-03-03 15:57:28,505 DEV : loss 1.2137300968170166 - score 0.3788\n",
            "Epoch    33: reducing learning rate of group 0 to 1.5625e-03.\n",
            "2022-03-03 15:57:28,577 BAD EPOCHS (no improvement): 4\n",
            "2022-03-03 15:57:28,579 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:57:29,474 epoch 34 - iter 1/19 - loss 0.99657947 - samples/sec: 161.85 - lr: 0.001563\n",
            "2022-03-03 15:57:29,591 epoch 34 - iter 2/19 - loss 0.96840104 - samples/sec: 499.58 - lr: 0.001563\n",
            "2022-03-03 15:57:29,675 epoch 34 - iter 3/19 - loss 0.94276828 - samples/sec: 443.89 - lr: 0.001563\n",
            "2022-03-03 15:57:29,750 epoch 34 - iter 4/19 - loss 0.90949596 - samples/sec: 457.82 - lr: 0.001563\n",
            "2022-03-03 15:57:29,825 epoch 34 - iter 5/19 - loss 0.89310656 - samples/sec: 439.43 - lr: 0.001563\n",
            "2022-03-03 15:57:29,893 epoch 34 - iter 6/19 - loss 0.87746628 - samples/sec: 500.17 - lr: 0.001563\n",
            "2022-03-03 15:57:29,968 epoch 34 - iter 7/19 - loss 0.87820221 - samples/sec: 488.86 - lr: 0.001563\n",
            "2022-03-03 15:57:30,042 epoch 34 - iter 8/19 - loss 0.87108070 - samples/sec: 526.25 - lr: 0.001563\n",
            "2022-03-03 15:57:30,109 epoch 34 - iter 9/19 - loss 0.88261041 - samples/sec: 497.23 - lr: 0.001563\n",
            "2022-03-03 15:57:30,181 epoch 34 - iter 10/19 - loss 0.87127825 - samples/sec: 451.88 - lr: 0.001563\n",
            "2022-03-03 15:57:30,253 epoch 34 - iter 11/19 - loss 0.87258805 - samples/sec: 459.99 - lr: 0.001563\n",
            "2022-03-03 15:57:30,321 epoch 34 - iter 12/19 - loss 0.87574510 - samples/sec: 498.17 - lr: 0.001563\n",
            "2022-03-03 15:57:30,390 epoch 34 - iter 13/19 - loss 0.87113442 - samples/sec: 474.53 - lr: 0.001563\n",
            "2022-03-03 15:57:30,466 epoch 34 - iter 14/19 - loss 0.86596133 - samples/sec: 501.02 - lr: 0.001563\n",
            "2022-03-03 15:57:30,536 epoch 34 - iter 15/19 - loss 0.86742050 - samples/sec: 510.63 - lr: 0.001563\n",
            "2022-03-03 15:57:30,616 epoch 34 - iter 16/19 - loss 0.86610774 - samples/sec: 499.26 - lr: 0.001563\n",
            "2022-03-03 15:57:30,688 epoch 34 - iter 17/19 - loss 0.87245935 - samples/sec: 500.39 - lr: 0.001563\n",
            "2022-03-03 15:57:30,762 epoch 34 - iter 18/19 - loss 0.88188415 - samples/sec: 505.01 - lr: 0.001563\n",
            "2022-03-03 15:57:30,826 epoch 34 - iter 19/19 - loss 0.88301854 - samples/sec: 550.36 - lr: 0.001563\n",
            "2022-03-03 15:57:31,226 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:57:31,227 EPOCH 34 done: loss 0.8830 - lr 0.0015625\n",
            "2022-03-03 15:57:32,475 DEV : loss 1.2135348320007324 - score 0.3788\n",
            "2022-03-03 15:57:32,549 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 15:57:32,551 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-03 15:57:33,467 epoch 35 - iter 1/19 - loss 0.85370606 - samples/sec: 126.31 - lr: 0.001563\n",
            "2022-03-03 15:57:33,568 epoch 35 - iter 2/19 - loss 0.82040930 - samples/sec: 444.95 - lr: 0.001563\n",
            "2022-03-03 15:57:33,638 epoch 35 - iter 3/19 - loss 0.87076163 - samples/sec: 482.75 - lr: 0.001563\n",
            "2022-03-03 15:57:33,746 epoch 35 - iter 4/19 - loss 0.88717796 - samples/sec: 478.89 - lr: 0.001563\n",
            "2022-03-03 15:57:33,824 epoch 35 - iter 5/19 - loss 0.86955711 - samples/sec: 453.77 - lr: 0.001563\n",
            "2022-03-03 15:57:33,899 epoch 35 - iter 6/19 - loss 0.89464004 - samples/sec: 443.87 - lr: 0.001563\n",
            "2022-03-03 15:57:33,975 epoch 35 - iter 7/19 - loss 0.89473896 - samples/sec: 428.41 - lr: 0.001563\n",
            "2022-03-03 15:57:34,056 epoch 35 - iter 8/19 - loss 0.89310330 - samples/sec: 446.42 - lr: 0.001563\n",
            "2022-03-03 15:57:34,125 epoch 35 - iter 9/19 - loss 0.87076981 - samples/sec: 478.08 - lr: 0.001563\n",
            "2022-03-03 15:57:34,197 epoch 35 - iter 10/19 - loss 0.86923011 - samples/sec: 541.08 - lr: 0.001563\n",
            "2022-03-03 15:57:34,266 epoch 35 - iter 11/19 - loss 0.86088188 - samples/sec: 481.28 - lr: 0.001563\n",
            "2022-03-03 15:57:34,332 epoch 35 - iter 12/19 - loss 0.85800955 - samples/sec: 494.32 - lr: 0.001563\n",
            "2022-03-03 15:57:34,397 epoch 35 - iter 13/19 - loss 0.86965640 - samples/sec: 509.53 - lr: 0.001563\n",
            "2022-03-03 15:57:34,475 epoch 35 - iter 14/19 - loss 0.88105530 - samples/sec: 421.13 - lr: 0.001563\n",
            "2022-03-03 15:57:34,558 epoch 35 - iter 15/19 - loss 0.87438849 - samples/sec: 411.87 - lr: 0.001563\n",
            "2022-03-03 15:57:36,176 epoch 35 - iter 16/19 - loss 0.87726367 - samples/sec: 598.62 - lr: 0.001563\n",
            "2022-03-03 15:57:36,250 epoch 35 - iter 17/19 - loss 0.86621369 - samples/sec: 442.40 - lr: 0.001563\n",
            "2022-03-03 15:57:36,314 epoch 35 - iter 18/19 - loss 0.87001485 - samples/sec: 569.75 - lr: 0.001563\n",
            "2022-03-03 15:57:36,364 epoch 35 - iter 19/19 - loss 0.87018428 - samples/sec: 687.64 - lr: 0.001563\n",
            "2022-03-03 15:57:36,795 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:57:36,797 EPOCH 35 done: loss 0.8702 - lr 0.0015625\n",
            "2022-03-03 15:57:38,024 DEV : loss 1.213000774383545 - score 0.3788\n",
            "2022-03-03 15:57:38,099 BAD EPOCHS (no improvement): 2\n",
            "2022-03-03 15:57:38,102 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:57:39,053 epoch 36 - iter 1/19 - loss 0.80014670 - samples/sec: 199.70 - lr: 0.001563\n",
            "2022-03-03 15:57:39,137 epoch 36 - iter 2/19 - loss 0.88402423 - samples/sec: 411.83 - lr: 0.001563\n",
            "2022-03-03 15:57:39,236 epoch 36 - iter 3/19 - loss 0.84944026 - samples/sec: 437.88 - lr: 0.001563\n",
            "2022-03-03 15:57:39,328 epoch 36 - iter 4/19 - loss 0.84274724 - samples/sec: 460.30 - lr: 0.001563\n",
            "2022-03-03 15:57:39,412 epoch 36 - iter 5/19 - loss 0.83452243 - samples/sec: 440.00 - lr: 0.001563\n",
            "2022-03-03 15:57:39,501 epoch 36 - iter 6/19 - loss 0.84801007 - samples/sec: 466.54 - lr: 0.001563\n",
            "2022-03-03 15:57:39,569 epoch 36 - iter 7/19 - loss 0.85943197 - samples/sec: 496.89 - lr: 0.001563\n",
            "2022-03-03 15:57:39,640 epoch 36 - iter 8/19 - loss 0.86176443 - samples/sec: 472.21 - lr: 0.001563\n",
            "2022-03-03 15:57:39,718 epoch 36 - iter 9/19 - loss 0.87333707 - samples/sec: 497.72 - lr: 0.001563\n",
            "2022-03-03 15:57:39,812 epoch 36 - iter 10/19 - loss 0.87061994 - samples/sec: 524.15 - lr: 0.001563\n",
            "2022-03-03 15:57:39,885 epoch 36 - iter 11/19 - loss 0.86293032 - samples/sec: 487.77 - lr: 0.001563\n",
            "2022-03-03 15:57:39,962 epoch 36 - iter 12/19 - loss 0.85844361 - samples/sec: 535.60 - lr: 0.001563\n",
            "2022-03-03 15:57:40,025 epoch 36 - iter 13/19 - loss 0.85608724 - samples/sec: 521.50 - lr: 0.001563\n",
            "2022-03-03 15:57:40,094 epoch 36 - iter 14/19 - loss 0.86316788 - samples/sec: 475.66 - lr: 0.001563\n",
            "2022-03-03 15:57:40,167 epoch 36 - iter 15/19 - loss 0.86681898 - samples/sec: 495.99 - lr: 0.001563\n",
            "2022-03-03 15:57:40,241 epoch 36 - iter 16/19 - loss 0.86489882 - samples/sec: 450.43 - lr: 0.001563\n",
            "2022-03-03 15:57:40,305 epoch 36 - iter 17/19 - loss 0.86633174 - samples/sec: 511.75 - lr: 0.001563\n",
            "2022-03-03 15:57:40,381 epoch 36 - iter 18/19 - loss 0.86952364 - samples/sec: 522.24 - lr: 0.001563\n",
            "2022-03-03 15:57:40,441 epoch 36 - iter 19/19 - loss 0.88110877 - samples/sec: 566.00 - lr: 0.001563\n",
            "2022-03-03 15:57:40,857 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:57:40,865 EPOCH 36 done: loss 0.8811 - lr 0.0015625\n",
            "2022-03-03 15:57:42,102 DEV : loss 1.2124146223068237 - score 0.3788\n",
            "2022-03-03 15:57:42,177 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:57:56,862 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:57:58,002 epoch 37 - iter 1/19 - loss 0.87241912 - samples/sec: 192.91 - lr: 0.001563\n",
            "2022-03-03 15:57:58,101 epoch 37 - iter 2/19 - loss 0.90825126 - samples/sec: 442.51 - lr: 0.001563\n",
            "2022-03-03 15:57:58,177 epoch 37 - iter 3/19 - loss 0.87557793 - samples/sec: 432.37 - lr: 0.001563\n",
            "2022-03-03 15:57:58,254 epoch 37 - iter 4/19 - loss 0.87528224 - samples/sec: 480.19 - lr: 0.001563\n",
            "2022-03-03 15:57:58,334 epoch 37 - iter 5/19 - loss 0.87803491 - samples/sec: 454.69 - lr: 0.001563\n",
            "2022-03-03 15:57:58,420 epoch 37 - iter 6/19 - loss 0.90754955 - samples/sec: 509.01 - lr: 0.001563\n",
            "2022-03-03 15:57:58,495 epoch 37 - iter 7/19 - loss 0.89032706 - samples/sec: 512.67 - lr: 0.001563\n",
            "2022-03-03 15:57:58,576 epoch 37 - iter 8/19 - loss 0.89939131 - samples/sec: 547.65 - lr: 0.001563\n",
            "2022-03-03 15:57:58,641 epoch 37 - iter 9/19 - loss 0.89571972 - samples/sec: 506.37 - lr: 0.001563\n",
            "2022-03-03 15:57:58,716 epoch 37 - iter 10/19 - loss 0.89003511 - samples/sec: 463.68 - lr: 0.001563\n",
            "2022-03-03 15:57:58,784 epoch 37 - iter 11/19 - loss 0.87772471 - samples/sec: 507.77 - lr: 0.001563\n",
            "2022-03-03 15:57:58,863 epoch 37 - iter 12/19 - loss 0.88876384 - samples/sec: 539.76 - lr: 0.001563\n",
            "2022-03-03 15:57:58,945 epoch 37 - iter 13/19 - loss 0.89088618 - samples/sec: 473.30 - lr: 0.001563\n",
            "2022-03-03 15:57:59,023 epoch 37 - iter 14/19 - loss 0.87478378 - samples/sec: 516.37 - lr: 0.001563\n",
            "2022-03-03 15:57:59,093 epoch 37 - iter 15/19 - loss 0.87831393 - samples/sec: 474.95 - lr: 0.001563\n",
            "2022-03-03 15:57:59,162 epoch 37 - iter 16/19 - loss 0.87780783 - samples/sec: 481.99 - lr: 0.001563\n",
            "2022-03-03 15:57:59,226 epoch 37 - iter 17/19 - loss 0.87544810 - samples/sec: 512.11 - lr: 0.001563\n",
            "2022-03-03 15:57:59,297 epoch 37 - iter 18/19 - loss 0.86916572 - samples/sec: 556.27 - lr: 0.001563\n",
            "2022-03-03 15:57:59,352 epoch 37 - iter 19/19 - loss 0.86331881 - samples/sec: 640.70 - lr: 0.001563\n",
            "2022-03-03 15:57:59,816 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:57:59,821 EPOCH 37 done: loss 0.8633 - lr 0.0015625\n",
            "2022-03-03 15:58:01,220 DEV : loss 1.212309718132019 - score 0.3788\n",
            "2022-03-03 15:58:01,295 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:58:15,665 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:58:16,814 epoch 38 - iter 1/19 - loss 0.79910827 - samples/sec: 245.81 - lr: 0.001563\n",
            "2022-03-03 15:58:16,905 epoch 38 - iter 2/19 - loss 0.79830778 - samples/sec: 425.08 - lr: 0.001563\n",
            "2022-03-03 15:58:16,983 epoch 38 - iter 3/19 - loss 0.80069788 - samples/sec: 432.14 - lr: 0.001563\n",
            "2022-03-03 15:58:17,057 epoch 38 - iter 4/19 - loss 0.79422918 - samples/sec: 481.49 - lr: 0.001563\n",
            "2022-03-03 15:58:17,133 epoch 38 - iter 5/19 - loss 0.83158022 - samples/sec: 518.94 - lr: 0.001563\n",
            "2022-03-03 15:58:17,226 epoch 38 - iter 6/19 - loss 0.82878491 - samples/sec: 445.89 - lr: 0.001563\n",
            "2022-03-03 15:58:17,304 epoch 38 - iter 7/19 - loss 0.82882328 - samples/sec: 421.24 - lr: 0.001563\n",
            "2022-03-03 15:58:17,372 epoch 38 - iter 8/19 - loss 0.83769647 - samples/sec: 481.07 - lr: 0.001563\n",
            "2022-03-03 15:58:17,455 epoch 38 - iter 9/19 - loss 0.84103464 - samples/sec: 514.36 - lr: 0.001563\n",
            "2022-03-03 15:58:17,527 epoch 38 - iter 10/19 - loss 0.85060165 - samples/sec: 472.92 - lr: 0.001563\n",
            "2022-03-03 15:58:17,602 epoch 38 - iter 11/19 - loss 0.83715831 - samples/sec: 542.07 - lr: 0.001563\n",
            "2022-03-03 15:58:17,679 epoch 38 - iter 12/19 - loss 0.84033704 - samples/sec: 534.41 - lr: 0.001563\n",
            "2022-03-03 15:58:17,748 epoch 38 - iter 13/19 - loss 0.83940144 - samples/sec: 508.81 - lr: 0.001563\n",
            "2022-03-03 15:58:17,818 epoch 38 - iter 14/19 - loss 0.83910664 - samples/sec: 475.95 - lr: 0.001563\n",
            "2022-03-03 15:58:17,895 epoch 38 - iter 15/19 - loss 0.83982661 - samples/sec: 548.37 - lr: 0.001563\n",
            "2022-03-03 15:58:17,966 epoch 38 - iter 16/19 - loss 0.84519158 - samples/sec: 465.52 - lr: 0.001563\n",
            "2022-03-03 15:58:18,044 epoch 38 - iter 17/19 - loss 0.86157959 - samples/sec: 477.69 - lr: 0.001563\n",
            "2022-03-03 15:58:18,118 epoch 38 - iter 18/19 - loss 0.86000910 - samples/sec: 493.67 - lr: 0.001563\n",
            "2022-03-03 15:58:18,175 epoch 38 - iter 19/19 - loss 0.86846300 - samples/sec: 717.74 - lr: 0.001563\n",
            "2022-03-03 15:58:18,646 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:58:18,648 EPOCH 38 done: loss 0.8685 - lr 0.0015625\n",
            "2022-03-03 15:58:20,058 DEV : loss 1.212066411972046 - score 0.3788\n",
            "2022-03-03 15:58:20,133 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:58:34,619 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:58:35,761 epoch 39 - iter 1/19 - loss 0.74592054 - samples/sec: 139.66 - lr: 0.001563\n",
            "2022-03-03 15:58:35,855 epoch 39 - iter 2/19 - loss 0.82142532 - samples/sec: 470.89 - lr: 0.001563\n",
            "2022-03-03 15:58:35,940 epoch 39 - iter 3/19 - loss 0.80758552 - samples/sec: 468.15 - lr: 0.001563\n",
            "2022-03-03 15:58:36,029 epoch 39 - iter 4/19 - loss 0.85856098 - samples/sec: 466.17 - lr: 0.001563\n",
            "2022-03-03 15:58:36,101 epoch 39 - iter 5/19 - loss 0.88656770 - samples/sec: 484.81 - lr: 0.001563\n",
            "2022-03-03 15:58:36,189 epoch 39 - iter 6/19 - loss 0.90261147 - samples/sec: 380.26 - lr: 0.001563\n",
            "2022-03-03 15:58:36,258 epoch 39 - iter 7/19 - loss 0.89668895 - samples/sec: 476.83 - lr: 0.001563\n",
            "2022-03-03 15:58:36,332 epoch 39 - iter 8/19 - loss 0.89669399 - samples/sec: 500.47 - lr: 0.001563\n",
            "2022-03-03 15:58:36,415 epoch 39 - iter 9/19 - loss 0.89461962 - samples/sec: 509.54 - lr: 0.001563\n",
            "2022-03-03 15:58:36,501 epoch 39 - iter 10/19 - loss 0.89909019 - samples/sec: 574.70 - lr: 0.001563\n",
            "2022-03-03 15:58:36,565 epoch 39 - iter 11/19 - loss 0.90235938 - samples/sec: 520.89 - lr: 0.001563\n",
            "2022-03-03 15:58:36,631 epoch 39 - iter 12/19 - loss 0.89838731 - samples/sec: 547.62 - lr: 0.001563\n",
            "2022-03-03 15:58:36,694 epoch 39 - iter 13/19 - loss 0.89622404 - samples/sec: 521.15 - lr: 0.001563\n",
            "2022-03-03 15:58:36,759 epoch 39 - iter 14/19 - loss 0.89549011 - samples/sec: 509.57 - lr: 0.001563\n",
            "2022-03-03 15:58:36,825 epoch 39 - iter 15/19 - loss 0.90032603 - samples/sec: 493.99 - lr: 0.001563\n",
            "2022-03-03 15:58:36,900 epoch 39 - iter 16/19 - loss 0.89857660 - samples/sec: 531.13 - lr: 0.001563\n",
            "2022-03-03 15:58:36,965 epoch 39 - iter 17/19 - loss 0.89308329 - samples/sec: 532.75 - lr: 0.001563\n",
            "2022-03-03 15:58:37,035 epoch 39 - iter 18/19 - loss 0.88798522 - samples/sec: 469.67 - lr: 0.001563\n",
            "2022-03-03 15:58:37,089 epoch 39 - iter 19/19 - loss 0.87976298 - samples/sec: 613.23 - lr: 0.001563\n",
            "2022-03-03 15:58:37,546 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:58:37,547 EPOCH 39 done: loss 0.8798 - lr 0.0015625\n",
            "2022-03-03 15:58:38,887 DEV : loss 1.2130155563354492 - score 0.3788\n",
            "2022-03-03 15:58:38,963 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 15:58:38,966 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:58:39,953 epoch 40 - iter 1/19 - loss 0.90973270 - samples/sec: 155.32 - lr: 0.001563\n",
            "2022-03-03 15:58:40,041 epoch 40 - iter 2/19 - loss 0.86082932 - samples/sec: 445.36 - lr: 0.001563\n",
            "2022-03-03 15:58:40,122 epoch 40 - iter 3/19 - loss 0.89476868 - samples/sec: 478.74 - lr: 0.001563\n",
            "2022-03-03 15:58:40,208 epoch 40 - iter 4/19 - loss 0.90686384 - samples/sec: 458.76 - lr: 0.001563\n",
            "2022-03-03 15:58:40,299 epoch 40 - iter 5/19 - loss 0.90959994 - samples/sec: 445.97 - lr: 0.001563\n",
            "2022-03-03 15:58:40,384 epoch 40 - iter 6/19 - loss 0.91496015 - samples/sec: 474.50 - lr: 0.001563\n",
            "2022-03-03 15:58:40,465 epoch 40 - iter 7/19 - loss 0.91745465 - samples/sec: 409.56 - lr: 0.001563\n",
            "2022-03-03 15:58:40,532 epoch 40 - iter 8/19 - loss 0.90561685 - samples/sec: 486.71 - lr: 0.001563\n",
            "2022-03-03 15:58:40,608 epoch 40 - iter 9/19 - loss 0.90868409 - samples/sec: 517.91 - lr: 0.001563\n",
            "2022-03-03 15:58:40,684 epoch 40 - iter 10/19 - loss 0.91317154 - samples/sec: 484.22 - lr: 0.001563\n",
            "2022-03-03 15:58:40,761 epoch 40 - iter 11/19 - loss 0.92080948 - samples/sec: 513.34 - lr: 0.001563\n",
            "2022-03-03 15:58:40,836 epoch 40 - iter 12/19 - loss 0.93201229 - samples/sec: 516.45 - lr: 0.001563\n",
            "2022-03-03 15:58:40,899 epoch 40 - iter 13/19 - loss 0.93106331 - samples/sec: 523.44 - lr: 0.001563\n",
            "2022-03-03 15:58:40,961 epoch 40 - iter 14/19 - loss 0.92357231 - samples/sec: 523.39 - lr: 0.001563\n",
            "2022-03-03 15:58:41,041 epoch 40 - iter 15/19 - loss 0.91451334 - samples/sec: 474.31 - lr: 0.001563\n",
            "2022-03-03 15:58:41,110 epoch 40 - iter 16/19 - loss 0.89530766 - samples/sec: 503.27 - lr: 0.001563\n",
            "2022-03-03 15:58:41,189 epoch 40 - iter 17/19 - loss 0.88473924 - samples/sec: 493.18 - lr: 0.001563\n",
            "2022-03-03 15:58:41,257 epoch 40 - iter 18/19 - loss 0.88010746 - samples/sec: 521.37 - lr: 0.001563\n",
            "2022-03-03 15:58:41,306 epoch 40 - iter 19/19 - loss 0.88568157 - samples/sec: 674.48 - lr: 0.001563\n",
            "2022-03-03 15:58:41,755 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:58:41,757 EPOCH 40 done: loss 0.8857 - lr 0.0015625\n",
            "2022-03-03 15:58:43,146 DEV : loss 1.2119983434677124 - score 0.3788\n",
            "2022-03-03 15:58:43,225 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:58:57,733 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:58:58,895 epoch 41 - iter 1/19 - loss 0.97398871 - samples/sec: 201.50 - lr: 0.001563\n",
            "2022-03-03 15:58:58,983 epoch 41 - iter 2/19 - loss 0.94043806 - samples/sec: 395.41 - lr: 0.001563\n",
            "2022-03-03 15:58:59,065 epoch 41 - iter 3/19 - loss 0.92372121 - samples/sec: 473.72 - lr: 0.001563\n",
            "2022-03-03 15:58:59,148 epoch 41 - iter 4/19 - loss 0.91305824 - samples/sec: 457.88 - lr: 0.001563\n",
            "2022-03-03 15:58:59,223 epoch 41 - iter 5/19 - loss 0.89170157 - samples/sec: 469.98 - lr: 0.001563\n",
            "2022-03-03 15:58:59,305 epoch 41 - iter 6/19 - loss 0.88071915 - samples/sec: 503.67 - lr: 0.001563\n",
            "2022-03-03 15:58:59,395 epoch 41 - iter 7/19 - loss 0.87350993 - samples/sec: 483.04 - lr: 0.001563\n",
            "2022-03-03 15:58:59,462 epoch 41 - iter 8/19 - loss 0.86838710 - samples/sec: 511.65 - lr: 0.001563\n",
            "2022-03-03 15:58:59,533 epoch 41 - iter 9/19 - loss 0.86841832 - samples/sec: 497.77 - lr: 0.001563\n",
            "2022-03-03 15:58:59,598 epoch 41 - iter 10/19 - loss 0.87187248 - samples/sec: 512.34 - lr: 0.001563\n",
            "2022-03-03 15:58:59,669 epoch 41 - iter 11/19 - loss 0.87185183 - samples/sec: 462.57 - lr: 0.001563\n",
            "2022-03-03 15:58:59,749 epoch 41 - iter 12/19 - loss 0.87943241 - samples/sec: 524.86 - lr: 0.001563\n",
            "2022-03-03 15:58:59,828 epoch 41 - iter 13/19 - loss 0.87597020 - samples/sec: 536.79 - lr: 0.001563\n",
            "2022-03-03 15:58:59,895 epoch 41 - iter 14/19 - loss 0.88020757 - samples/sec: 493.14 - lr: 0.001563\n",
            "2022-03-03 15:58:59,971 epoch 41 - iter 15/19 - loss 0.86878415 - samples/sec: 479.11 - lr: 0.001563\n",
            "2022-03-03 15:59:00,040 epoch 41 - iter 16/19 - loss 0.87449137 - samples/sec: 499.95 - lr: 0.001563\n",
            "2022-03-03 15:59:00,106 epoch 41 - iter 17/19 - loss 0.87159247 - samples/sec: 498.99 - lr: 0.001563\n",
            "2022-03-03 15:59:00,179 epoch 41 - iter 18/19 - loss 0.87004348 - samples/sec: 460.71 - lr: 0.001563\n",
            "2022-03-03 15:59:00,236 epoch 41 - iter 19/19 - loss 0.87848254 - samples/sec: 609.34 - lr: 0.001563\n",
            "2022-03-03 15:59:00,673 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:59:00,675 EPOCH 41 done: loss 0.8785 - lr 0.0015625\n",
            "2022-03-03 15:59:02,075 DEV : loss 1.2114837169647217 - score 0.3788\n",
            "2022-03-03 15:59:02,150 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:59:16,750 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:59:17,932 epoch 42 - iter 1/19 - loss 0.88207525 - samples/sec: 228.76 - lr: 0.001563\n",
            "2022-03-03 15:59:18,031 epoch 42 - iter 2/19 - loss 0.82986468 - samples/sec: 440.64 - lr: 0.001563\n",
            "2022-03-03 15:59:18,104 epoch 42 - iter 3/19 - loss 0.83426843 - samples/sec: 450.69 - lr: 0.001563\n",
            "2022-03-03 15:59:18,187 epoch 42 - iter 4/19 - loss 0.84327944 - samples/sec: 466.39 - lr: 0.001563\n",
            "2022-03-03 15:59:18,256 epoch 42 - iter 5/19 - loss 0.86571602 - samples/sec: 483.30 - lr: 0.001563\n",
            "2022-03-03 15:59:18,331 epoch 42 - iter 6/19 - loss 0.87237231 - samples/sec: 533.11 - lr: 0.001563\n",
            "2022-03-03 15:59:18,414 epoch 42 - iter 7/19 - loss 0.86727858 - samples/sec: 496.59 - lr: 0.001563\n",
            "2022-03-03 15:59:18,488 epoch 42 - iter 8/19 - loss 0.87896764 - samples/sec: 533.29 - lr: 0.001563\n",
            "2022-03-03 15:59:18,554 epoch 42 - iter 9/19 - loss 0.85618271 - samples/sec: 501.13 - lr: 0.001563\n",
            "2022-03-03 15:59:18,649 epoch 42 - iter 10/19 - loss 0.86075086 - samples/sec: 382.06 - lr: 0.001563\n",
            "2022-03-03 15:59:18,717 epoch 42 - iter 11/19 - loss 0.86719901 - samples/sec: 484.84 - lr: 0.001563\n",
            "2022-03-03 15:59:18,789 epoch 42 - iter 12/19 - loss 0.86811993 - samples/sec: 551.10 - lr: 0.001563\n",
            "2022-03-03 15:59:18,865 epoch 42 - iter 13/19 - loss 0.87827213 - samples/sec: 526.32 - lr: 0.001563\n",
            "2022-03-03 15:59:18,946 epoch 42 - iter 14/19 - loss 0.87670531 - samples/sec: 510.97 - lr: 0.001563\n",
            "2022-03-03 15:59:19,018 epoch 42 - iter 15/19 - loss 0.87734391 - samples/sec: 494.02 - lr: 0.001563\n",
            "2022-03-03 15:59:19,083 epoch 42 - iter 16/19 - loss 0.88144343 - samples/sec: 506.11 - lr: 0.001563\n",
            "2022-03-03 15:59:19,151 epoch 42 - iter 17/19 - loss 0.88112704 - samples/sec: 517.25 - lr: 0.001563\n",
            "2022-03-03 15:59:19,231 epoch 42 - iter 18/19 - loss 0.87282315 - samples/sec: 410.68 - lr: 0.001563\n",
            "2022-03-03 15:59:20,817 epoch 42 - iter 19/19 - loss 0.87189543 - samples/sec: 664.03 - lr: 0.001563\n",
            "2022-03-03 15:59:21,288 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:59:21,294 EPOCH 42 done: loss 0.8719 - lr 0.0015625\n",
            "2022-03-03 15:59:22,798 DEV : loss 1.211453914642334 - score 0.3788\n",
            "2022-03-03 15:59:22,872 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:59:37,057 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:59:38,264 epoch 43 - iter 1/19 - loss 0.90556347 - samples/sec: 172.37 - lr: 0.001563\n",
            "2022-03-03 15:59:38,352 epoch 43 - iter 2/19 - loss 0.86799276 - samples/sec: 453.89 - lr: 0.001563\n",
            "2022-03-03 15:59:38,454 epoch 43 - iter 3/19 - loss 0.86413721 - samples/sec: 404.53 - lr: 0.001563\n",
            "2022-03-03 15:59:38,533 epoch 43 - iter 4/19 - loss 0.85780655 - samples/sec: 429.03 - lr: 0.001563\n",
            "2022-03-03 15:59:38,613 epoch 43 - iter 5/19 - loss 0.86182052 - samples/sec: 447.03 - lr: 0.001563\n",
            "2022-03-03 15:59:38,713 epoch 43 - iter 6/19 - loss 0.85161354 - samples/sec: 440.03 - lr: 0.001563\n",
            "2022-03-03 15:59:38,785 epoch 43 - iter 7/19 - loss 0.83651769 - samples/sec: 460.78 - lr: 0.001563\n",
            "2022-03-03 15:59:38,868 epoch 43 - iter 8/19 - loss 0.85085949 - samples/sec: 531.54 - lr: 0.001563\n",
            "2022-03-03 15:59:38,944 epoch 43 - iter 9/19 - loss 0.87069869 - samples/sec: 463.26 - lr: 0.001563\n",
            "2022-03-03 15:59:39,014 epoch 43 - iter 10/19 - loss 0.85914861 - samples/sec: 472.60 - lr: 0.001563\n",
            "2022-03-03 15:59:39,081 epoch 43 - iter 11/19 - loss 0.86280648 - samples/sec: 487.80 - lr: 0.001563\n",
            "2022-03-03 15:59:39,161 epoch 43 - iter 12/19 - loss 0.86600942 - samples/sec: 558.48 - lr: 0.001563\n",
            "2022-03-03 15:59:39,233 epoch 43 - iter 13/19 - loss 0.86333186 - samples/sec: 457.26 - lr: 0.001563\n",
            "2022-03-03 15:59:39,308 epoch 43 - iter 14/19 - loss 0.86727239 - samples/sec: 549.24 - lr: 0.001563\n",
            "2022-03-03 15:59:39,378 epoch 43 - iter 15/19 - loss 0.86614094 - samples/sec: 508.81 - lr: 0.001563\n",
            "2022-03-03 15:59:39,446 epoch 43 - iter 16/19 - loss 0.87229144 - samples/sec: 478.03 - lr: 0.001563\n",
            "2022-03-03 15:59:39,515 epoch 43 - iter 17/19 - loss 0.87324789 - samples/sec: 507.51 - lr: 0.001563\n",
            "2022-03-03 15:59:39,588 epoch 43 - iter 18/19 - loss 0.87387068 - samples/sec: 555.07 - lr: 0.001563\n",
            "2022-03-03 15:59:39,640 epoch 43 - iter 19/19 - loss 0.87333661 - samples/sec: 649.82 - lr: 0.001563\n",
            "2022-03-03 15:59:40,094 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:59:40,096 EPOCH 43 done: loss 0.8733 - lr 0.0015625\n",
            "2022-03-03 15:59:41,516 DEV : loss 1.2111058235168457 - score 0.3788\n",
            "2022-03-03 15:59:41,604 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 15:59:55,696 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 15:59:56,842 epoch 44 - iter 1/19 - loss 0.95929748 - samples/sec: 159.94 - lr: 0.001563\n",
            "2022-03-03 15:59:56,959 epoch 44 - iter 2/19 - loss 0.91506085 - samples/sec: 459.39 - lr: 0.001563\n",
            "2022-03-03 15:59:57,034 epoch 44 - iter 3/19 - loss 0.88041123 - samples/sec: 439.89 - lr: 0.001563\n",
            "2022-03-03 15:59:57,134 epoch 44 - iter 4/19 - loss 0.89910488 - samples/sec: 381.25 - lr: 0.001563\n",
            "2022-03-03 15:59:57,207 epoch 44 - iter 5/19 - loss 0.89296881 - samples/sec: 447.73 - lr: 0.001563\n",
            "2022-03-03 15:59:57,290 epoch 44 - iter 6/19 - loss 0.87840760 - samples/sec: 445.86 - lr: 0.001563\n",
            "2022-03-03 15:59:57,371 epoch 44 - iter 7/19 - loss 0.87881256 - samples/sec: 402.02 - lr: 0.001563\n",
            "2022-03-03 15:59:57,451 epoch 44 - iter 8/19 - loss 0.88225199 - samples/sec: 537.89 - lr: 0.001563\n",
            "2022-03-03 15:59:57,518 epoch 44 - iter 9/19 - loss 0.87106479 - samples/sec: 497.26 - lr: 0.001563\n",
            "2022-03-03 15:59:57,579 epoch 44 - iter 10/19 - loss 0.87602485 - samples/sec: 532.99 - lr: 0.001563\n",
            "2022-03-03 15:59:57,647 epoch 44 - iter 11/19 - loss 0.87337520 - samples/sec: 484.64 - lr: 0.001563\n",
            "2022-03-03 15:59:57,724 epoch 44 - iter 12/19 - loss 0.87253290 - samples/sec: 553.09 - lr: 0.001563\n",
            "2022-03-03 15:59:57,798 epoch 44 - iter 13/19 - loss 0.87118031 - samples/sec: 444.76 - lr: 0.001563\n",
            "2022-03-03 15:59:57,884 epoch 44 - iter 14/19 - loss 0.87288045 - samples/sec: 522.82 - lr: 0.001563\n",
            "2022-03-03 15:59:57,954 epoch 44 - iter 15/19 - loss 0.88035388 - samples/sec: 563.14 - lr: 0.001563\n",
            "2022-03-03 15:59:58,020 epoch 44 - iter 16/19 - loss 0.87463672 - samples/sec: 506.09 - lr: 0.001563\n",
            "2022-03-03 15:59:58,092 epoch 44 - iter 17/19 - loss 0.86943885 - samples/sec: 453.59 - lr: 0.001563\n",
            "2022-03-03 15:59:58,166 epoch 44 - iter 18/19 - loss 0.87592889 - samples/sec: 441.15 - lr: 0.001563\n",
            "2022-03-03 15:59:58,220 epoch 44 - iter 19/19 - loss 0.86928781 - samples/sec: 635.72 - lr: 0.001563\n",
            "2022-03-03 15:59:58,680 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 15:59:58,682 EPOCH 44 done: loss 0.8693 - lr 0.0015625\n",
            "2022-03-03 16:00:00,092 DEV : loss 1.2110875844955444 - score 0.3788\n",
            "2022-03-03 16:00:00,167 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 16:00:14,550 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 16:00:15,676 epoch 45 - iter 1/19 - loss 0.99288613 - samples/sec: 177.40 - lr: 0.001563\n",
            "2022-03-03 16:00:15,758 epoch 45 - iter 2/19 - loss 0.95952234 - samples/sec: 483.12 - lr: 0.001563\n",
            "2022-03-03 16:00:15,837 epoch 45 - iter 3/19 - loss 0.92259198 - samples/sec: 482.23 - lr: 0.001563\n",
            "2022-03-03 16:00:15,933 epoch 45 - iter 4/19 - loss 0.88998225 - samples/sec: 492.62 - lr: 0.001563\n",
            "2022-03-03 16:00:16,034 epoch 45 - iter 5/19 - loss 0.87889303 - samples/sec: 466.55 - lr: 0.001563\n",
            "2022-03-03 16:00:16,117 epoch 45 - iter 6/19 - loss 0.87136970 - samples/sec: 488.53 - lr: 0.001563\n",
            "2022-03-03 16:00:16,192 epoch 45 - iter 7/19 - loss 0.86179695 - samples/sec: 459.23 - lr: 0.001563\n",
            "2022-03-03 16:00:16,257 epoch 45 - iter 8/19 - loss 0.86418530 - samples/sec: 498.94 - lr: 0.001563\n",
            "2022-03-03 16:00:16,327 epoch 45 - iter 9/19 - loss 0.86914361 - samples/sec: 467.98 - lr: 0.001563\n",
            "2022-03-03 16:00:16,395 epoch 45 - iter 10/19 - loss 0.85754252 - samples/sec: 488.43 - lr: 0.001563\n",
            "2022-03-03 16:00:16,473 epoch 45 - iter 11/19 - loss 0.87281546 - samples/sec: 530.33 - lr: 0.001563\n",
            "2022-03-03 16:00:16,544 epoch 45 - iter 12/19 - loss 0.87170080 - samples/sec: 519.31 - lr: 0.001563\n",
            "2022-03-03 16:00:16,606 epoch 45 - iter 13/19 - loss 0.87162727 - samples/sec: 532.02 - lr: 0.001563\n",
            "2022-03-03 16:00:16,672 epoch 45 - iter 14/19 - loss 0.87718611 - samples/sec: 544.47 - lr: 0.001563\n",
            "2022-03-03 16:00:16,743 epoch 45 - iter 15/19 - loss 0.87242912 - samples/sec: 467.16 - lr: 0.001563\n",
            "2022-03-03 16:00:16,809 epoch 45 - iter 16/19 - loss 0.87603229 - samples/sec: 496.07 - lr: 0.001563\n",
            "2022-03-03 16:00:16,909 epoch 45 - iter 17/19 - loss 0.87982724 - samples/sec: 382.83 - lr: 0.001563\n",
            "2022-03-03 16:00:16,981 epoch 45 - iter 18/19 - loss 0.88094981 - samples/sec: 460.27 - lr: 0.001563\n",
            "2022-03-03 16:00:17,034 epoch 45 - iter 19/19 - loss 0.87928912 - samples/sec: 738.44 - lr: 0.001563\n",
            "2022-03-03 16:00:17,488 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:00:17,490 EPOCH 45 done: loss 0.8793 - lr 0.0015625\n",
            "2022-03-03 16:00:18,928 DEV : loss 1.2105443477630615 - score 0.3788\n",
            "2022-03-03 16:00:19,006 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 16:00:33,202 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 16:00:34,348 epoch 46 - iter 1/19 - loss 0.95835292 - samples/sec: 215.57 - lr: 0.001563\n",
            "2022-03-03 16:00:34,441 epoch 46 - iter 2/19 - loss 0.95723498 - samples/sec: 460.37 - lr: 0.001563\n",
            "2022-03-03 16:00:34,532 epoch 46 - iter 3/19 - loss 0.93962197 - samples/sec: 407.56 - lr: 0.001563\n",
            "2022-03-03 16:00:34,613 epoch 46 - iter 4/19 - loss 0.93321471 - samples/sec: 411.30 - lr: 0.001563\n",
            "2022-03-03 16:00:34,690 epoch 46 - iter 5/19 - loss 0.97028202 - samples/sec: 428.98 - lr: 0.001563\n",
            "2022-03-03 16:00:34,787 epoch 46 - iter 6/19 - loss 0.92473882 - samples/sec: 441.03 - lr: 0.001563\n",
            "2022-03-03 16:00:34,860 epoch 46 - iter 7/19 - loss 0.92035082 - samples/sec: 508.39 - lr: 0.001563\n",
            "2022-03-03 16:00:34,939 epoch 46 - iter 8/19 - loss 0.92006579 - samples/sec: 536.92 - lr: 0.001563\n",
            "2022-03-03 16:00:35,009 epoch 46 - iter 9/19 - loss 0.90843866 - samples/sec: 550.45 - lr: 0.001563\n",
            "2022-03-03 16:00:35,078 epoch 46 - iter 10/19 - loss 0.90569418 - samples/sec: 474.53 - lr: 0.001563\n",
            "2022-03-03 16:00:35,149 epoch 46 - iter 11/19 - loss 0.89767098 - samples/sec: 483.66 - lr: 0.001563\n",
            "2022-03-03 16:00:35,233 epoch 46 - iter 12/19 - loss 0.89737581 - samples/sec: 498.07 - lr: 0.001563\n",
            "2022-03-03 16:00:35,300 epoch 46 - iter 13/19 - loss 0.89737183 - samples/sec: 490.61 - lr: 0.001563\n",
            "2022-03-03 16:00:35,375 epoch 46 - iter 14/19 - loss 0.89364813 - samples/sec: 544.73 - lr: 0.001563\n",
            "2022-03-03 16:00:35,443 epoch 46 - iter 15/19 - loss 0.88043481 - samples/sec: 568.32 - lr: 0.001563\n",
            "2022-03-03 16:00:35,522 epoch 46 - iter 16/19 - loss 0.88247344 - samples/sec: 417.70 - lr: 0.001563\n",
            "2022-03-03 16:00:35,590 epoch 46 - iter 17/19 - loss 0.87846694 - samples/sec: 500.91 - lr: 0.001563\n",
            "2022-03-03 16:00:35,671 epoch 46 - iter 18/19 - loss 0.87822330 - samples/sec: 490.16 - lr: 0.001563\n",
            "2022-03-03 16:00:35,725 epoch 46 - iter 19/19 - loss 0.88086427 - samples/sec: 610.56 - lr: 0.001563\n",
            "2022-03-03 16:00:36,184 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:00:36,186 EPOCH 46 done: loss 0.8809 - lr 0.0015625\n",
            "2022-03-03 16:00:37,588 DEV : loss 1.210547924041748 - score 0.3788\n",
            "2022-03-03 16:00:37,674 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 16:00:37,677 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 16:00:38,694 epoch 47 - iter 1/19 - loss 0.90378553 - samples/sec: 220.41 - lr: 0.001563\n",
            "2022-03-03 16:00:38,776 epoch 47 - iter 2/19 - loss 0.81980839 - samples/sec: 485.97 - lr: 0.001563\n",
            "2022-03-03 16:00:38,857 epoch 47 - iter 3/19 - loss 0.83979853 - samples/sec: 458.68 - lr: 0.001563\n",
            "2022-03-03 16:00:38,942 epoch 47 - iter 4/19 - loss 0.81443259 - samples/sec: 510.25 - lr: 0.001563\n",
            "2022-03-03 16:00:39,022 epoch 47 - iter 5/19 - loss 0.81387535 - samples/sec: 453.72 - lr: 0.001563\n",
            "2022-03-03 16:00:39,124 epoch 47 - iter 6/19 - loss 0.81144486 - samples/sec: 462.70 - lr: 0.001563\n",
            "2022-03-03 16:00:39,204 epoch 47 - iter 7/19 - loss 0.81105848 - samples/sec: 468.83 - lr: 0.001563\n",
            "2022-03-03 16:00:39,268 epoch 47 - iter 8/19 - loss 0.83508977 - samples/sec: 509.83 - lr: 0.001563\n",
            "2022-03-03 16:00:39,331 epoch 47 - iter 9/19 - loss 0.85027645 - samples/sec: 528.63 - lr: 0.001563\n",
            "2022-03-03 16:00:39,401 epoch 47 - iter 10/19 - loss 0.85925276 - samples/sec: 470.23 - lr: 0.001563\n",
            "2022-03-03 16:00:39,470 epoch 47 - iter 11/19 - loss 0.85307996 - samples/sec: 506.64 - lr: 0.001563\n",
            "2022-03-03 16:00:39,555 epoch 47 - iter 12/19 - loss 0.86112919 - samples/sec: 556.59 - lr: 0.001563\n",
            "2022-03-03 16:00:39,619 epoch 47 - iter 13/19 - loss 0.86454686 - samples/sec: 522.51 - lr: 0.001563\n",
            "2022-03-03 16:00:39,686 epoch 47 - iter 14/19 - loss 0.87357846 - samples/sec: 484.09 - lr: 0.001563\n",
            "2022-03-03 16:00:39,758 epoch 47 - iter 15/19 - loss 0.86428615 - samples/sec: 468.59 - lr: 0.001563\n",
            "2022-03-03 16:00:39,824 epoch 47 - iter 16/19 - loss 0.86603560 - samples/sec: 497.98 - lr: 0.001563\n",
            "2022-03-03 16:00:39,901 epoch 47 - iter 17/19 - loss 0.86480968 - samples/sec: 503.04 - lr: 0.001563\n",
            "2022-03-03 16:00:39,976 epoch 47 - iter 18/19 - loss 0.87230384 - samples/sec: 508.79 - lr: 0.001563\n",
            "2022-03-03 16:00:40,025 epoch 47 - iter 19/19 - loss 0.87216463 - samples/sec: 685.04 - lr: 0.001563\n",
            "2022-03-03 16:00:40,493 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:00:40,495 EPOCH 47 done: loss 0.8722 - lr 0.0015625\n",
            "2022-03-03 16:00:41,898 DEV : loss 1.2100213766098022 - score 0.3788\n",
            "2022-03-03 16:00:41,974 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 16:00:56,213 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 16:00:57,348 epoch 48 - iter 1/19 - loss 0.87427998 - samples/sec: 136.95 - lr: 0.001563\n",
            "2022-03-03 16:00:57,447 epoch 48 - iter 2/19 - loss 0.90017366 - samples/sec: 460.15 - lr: 0.001563\n",
            "2022-03-03 16:00:57,530 epoch 48 - iter 3/19 - loss 0.84495286 - samples/sec: 524.51 - lr: 0.001563\n",
            "2022-03-03 16:00:57,633 epoch 48 - iter 4/19 - loss 0.85458595 - samples/sec: 469.27 - lr: 0.001563\n",
            "2022-03-03 16:00:57,715 epoch 48 - iter 5/19 - loss 0.84732442 - samples/sec: 430.21 - lr: 0.001563\n",
            "2022-03-03 16:00:57,787 epoch 48 - iter 6/19 - loss 0.88441835 - samples/sec: 473.55 - lr: 0.001563\n",
            "2022-03-03 16:00:57,863 epoch 48 - iter 7/19 - loss 0.89844644 - samples/sec: 473.36 - lr: 0.001563\n",
            "2022-03-03 16:00:57,926 epoch 48 - iter 8/19 - loss 0.87324100 - samples/sec: 526.07 - lr: 0.001563\n",
            "2022-03-03 16:00:57,999 epoch 48 - iter 9/19 - loss 0.87145241 - samples/sec: 447.06 - lr: 0.001563\n",
            "2022-03-03 16:00:58,071 epoch 48 - iter 10/19 - loss 0.86062385 - samples/sec: 492.62 - lr: 0.001563\n",
            "2022-03-03 16:00:58,143 epoch 48 - iter 11/19 - loss 0.86760428 - samples/sec: 458.03 - lr: 0.001563\n",
            "2022-03-03 16:00:58,226 epoch 48 - iter 12/19 - loss 0.88376757 - samples/sec: 502.34 - lr: 0.001563\n",
            "2022-03-03 16:00:58,306 epoch 48 - iter 13/19 - loss 0.87953781 - samples/sec: 407.29 - lr: 0.001563\n",
            "2022-03-03 16:00:58,379 epoch 48 - iter 14/19 - loss 0.87377413 - samples/sec: 452.20 - lr: 0.001563\n",
            "2022-03-03 16:00:58,451 epoch 48 - iter 15/19 - loss 0.86592180 - samples/sec: 551.79 - lr: 0.001563\n",
            "2022-03-03 16:00:58,522 epoch 48 - iter 16/19 - loss 0.85943966 - samples/sec: 513.28 - lr: 0.001563\n",
            "2022-03-03 16:00:58,594 epoch 48 - iter 17/19 - loss 0.86015693 - samples/sec: 511.01 - lr: 0.001563\n",
            "2022-03-03 16:00:58,665 epoch 48 - iter 18/19 - loss 0.86544366 - samples/sec: 558.50 - lr: 0.001563\n",
            "2022-03-03 16:00:58,716 epoch 48 - iter 19/19 - loss 0.88202920 - samples/sec: 653.83 - lr: 0.001563\n",
            "2022-03-03 16:00:59,236 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:00:59,241 EPOCH 48 done: loss 0.8820 - lr 0.0015625\n",
            "2022-03-03 16:01:00,741 DEV : loss 1.2092607021331787 - score 0.3788\n",
            "2022-03-03 16:01:00,818 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2022-03-03 16:01:14,977 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 16:01:16,110 epoch 49 - iter 1/19 - loss 0.79435778 - samples/sec: 217.52 - lr: 0.001563\n",
            "2022-03-03 16:01:16,189 epoch 49 - iter 2/19 - loss 0.82226458 - samples/sec: 426.89 - lr: 0.001563\n",
            "2022-03-03 16:01:16,273 epoch 49 - iter 3/19 - loss 0.79784018 - samples/sec: 454.23 - lr: 0.001563\n",
            "2022-03-03 16:01:16,360 epoch 49 - iter 4/19 - loss 0.81237859 - samples/sec: 464.39 - lr: 0.001563\n",
            "2022-03-03 16:01:16,444 epoch 49 - iter 5/19 - loss 0.82742730 - samples/sec: 465.35 - lr: 0.001563\n",
            "2022-03-03 16:01:18,032 epoch 49 - iter 6/19 - loss 0.85486746 - samples/sec: 468.66 - lr: 0.001563\n",
            "2022-03-03 16:01:18,109 epoch 49 - iter 7/19 - loss 0.87662508 - samples/sec: 474.83 - lr: 0.001563\n",
            "2022-03-03 16:01:18,176 epoch 49 - iter 8/19 - loss 0.89531146 - samples/sec: 513.45 - lr: 0.001563\n",
            "2022-03-03 16:01:18,237 epoch 49 - iter 9/19 - loss 0.88189930 - samples/sec: 536.60 - lr: 0.001563\n",
            "2022-03-03 16:01:18,301 epoch 49 - iter 10/19 - loss 0.89164787 - samples/sec: 520.36 - lr: 0.001563\n",
            "2022-03-03 16:01:18,368 epoch 49 - iter 11/19 - loss 0.88207514 - samples/sec: 505.30 - lr: 0.001563\n",
            "2022-03-03 16:01:18,438 epoch 49 - iter 12/19 - loss 0.88773339 - samples/sec: 536.01 - lr: 0.001563\n",
            "2022-03-03 16:01:18,509 epoch 49 - iter 13/19 - loss 0.86957268 - samples/sec: 504.19 - lr: 0.001563\n",
            "2022-03-03 16:01:18,579 epoch 49 - iter 14/19 - loss 0.86591569 - samples/sec: 527.91 - lr: 0.001563\n",
            "2022-03-03 16:01:18,643 epoch 49 - iter 15/19 - loss 0.85966383 - samples/sec: 552.68 - lr: 0.001563\n",
            "2022-03-03 16:01:18,710 epoch 49 - iter 16/19 - loss 0.85870308 - samples/sec: 610.71 - lr: 0.001563\n",
            "2022-03-03 16:01:18,781 epoch 49 - iter 17/19 - loss 0.86997581 - samples/sec: 501.79 - lr: 0.001563\n",
            "2022-03-03 16:01:18,854 epoch 49 - iter 18/19 - loss 0.87493548 - samples/sec: 494.28 - lr: 0.001563\n",
            "2022-03-03 16:01:18,919 epoch 49 - iter 19/19 - loss 0.87040871 - samples/sec: 653.29 - lr: 0.001563\n",
            "2022-03-03 16:01:19,401 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:01:19,403 EPOCH 49 done: loss 0.8704 - lr 0.0015625\n",
            "2022-03-03 16:01:20,914 DEV : loss 1.2093929052352905 - score 0.3788\n",
            "2022-03-03 16:01:20,993 BAD EPOCHS (no improvement): 1\n",
            "2022-03-03 16:01:20,997 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 16:01:22,004 epoch 50 - iter 1/19 - loss 0.80365598 - samples/sec: 129.86 - lr: 0.001563\n",
            "2022-03-03 16:01:22,141 epoch 50 - iter 2/19 - loss 0.81334925 - samples/sec: 361.36 - lr: 0.001563\n",
            "2022-03-03 16:01:22,224 epoch 50 - iter 3/19 - loss 0.77830293 - samples/sec: 406.42 - lr: 0.001563\n",
            "2022-03-03 16:01:22,306 epoch 50 - iter 4/19 - loss 0.80647467 - samples/sec: 504.86 - lr: 0.001563\n",
            "2022-03-03 16:01:22,379 epoch 50 - iter 5/19 - loss 0.83301222 - samples/sec: 479.41 - lr: 0.001563\n",
            "2022-03-03 16:01:22,457 epoch 50 - iter 6/19 - loss 0.84717235 - samples/sec: 475.20 - lr: 0.001563\n",
            "2022-03-03 16:01:22,536 epoch 50 - iter 7/19 - loss 0.84552442 - samples/sec: 449.76 - lr: 0.001563\n",
            "2022-03-03 16:01:22,610 epoch 50 - iter 8/19 - loss 0.86090840 - samples/sec: 443.70 - lr: 0.001563\n",
            "2022-03-03 16:01:22,677 epoch 50 - iter 9/19 - loss 0.85523418 - samples/sec: 499.41 - lr: 0.001563\n",
            "2022-03-03 16:01:22,760 epoch 50 - iter 10/19 - loss 0.86312727 - samples/sec: 548.38 - lr: 0.001563\n",
            "2022-03-03 16:01:22,831 epoch 50 - iter 11/19 - loss 0.86535288 - samples/sec: 467.10 - lr: 0.001563\n",
            "2022-03-03 16:01:22,898 epoch 50 - iter 12/19 - loss 0.84916497 - samples/sec: 489.81 - lr: 0.001563\n",
            "2022-03-03 16:01:22,971 epoch 50 - iter 13/19 - loss 0.84637321 - samples/sec: 448.50 - lr: 0.001563\n",
            "2022-03-03 16:01:23,046 epoch 50 - iter 14/19 - loss 0.85454440 - samples/sec: 440.99 - lr: 0.001563\n",
            "2022-03-03 16:01:23,115 epoch 50 - iter 15/19 - loss 0.85269525 - samples/sec: 474.29 - lr: 0.001563\n",
            "2022-03-03 16:01:23,209 epoch 50 - iter 16/19 - loss 0.85563023 - samples/sec: 458.84 - lr: 0.001563\n",
            "2022-03-03 16:01:23,274 epoch 50 - iter 17/19 - loss 0.86021730 - samples/sec: 510.72 - lr: 0.001563\n",
            "2022-03-03 16:01:23,341 epoch 50 - iter 18/19 - loss 0.86342748 - samples/sec: 488.41 - lr: 0.001563\n",
            "2022-03-03 16:01:23,395 epoch 50 - iter 19/19 - loss 0.86045887 - samples/sec: 616.59 - lr: 0.001563\n",
            "2022-03-03 16:01:23,835 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:01:23,837 EPOCH 50 done: loss 0.8605 - lr 0.0015625\n",
            "2022-03-03 16:01:25,237 DEV : loss 1.2094271183013916 - score 0.3788\n",
            "2022-03-03 16:01:25,324 BAD EPOCHS (no improvement): 2\n",
            "2022-03-03 16:01:39,800 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:01:39,829 Testing using best model ...\n",
            "2022-03-03 16:01:39,831 loading file flair-feminism-model/best-model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 16:01:53,047 \t0.6316\n",
            "2022-03-03 16:01:53,051 \n",
            "Results:\n",
            "- F-score (micro) 0.6316\n",
            "- F-score (macro) 0.3974\n",
            "- Accuracy 0.6316\n",
            "\n",
            "By class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       FAVOR     0.3913    0.4655    0.4252        58\n",
            "     AGAINST     0.7083    0.8361    0.7669       183\n",
            "        NONE     0.0000    0.0000    0.0000        44\n",
            "\n",
            "   micro avg     0.6316    0.6316    0.6316       285\n",
            "   macro avg     0.3665    0.4339    0.3974       285\n",
            "weighted avg     0.5345    0.6316    0.5790       285\n",
            " samples avg     0.6316    0.6316    0.6316       285\n",
            "\n",
            "2022-03-03 16:01:53,053 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [1.441864013671875,\n",
              "  1.3048807382583618,\n",
              "  1.3046139478683472,\n",
              "  1.307172179222107,\n",
              "  1.2645920515060425,\n",
              "  1.3059132099151611,\n",
              "  1.2765865325927734,\n",
              "  1.2544654607772827,\n",
              "  1.2129359245300293,\n",
              "  1.2704132795333862,\n",
              "  1.2664158344268799,\n",
              "  1.2114628553390503,\n",
              "  1.2053146362304688,\n",
              "  1.2317249774932861,\n",
              "  1.2290961742401123,\n",
              "  1.2123174667358398,\n",
              "  1.268858551979065,\n",
              "  1.23180091381073,\n",
              "  1.2090182304382324,\n",
              "  1.207627534866333,\n",
              "  1.2254174947738647,\n",
              "  1.2183997631072998,\n",
              "  1.2187011241912842,\n",
              "  1.214099407196045,\n",
              "  1.2191228866577148,\n",
              "  1.218406081199646,\n",
              "  1.2156246900558472,\n",
              "  1.2144984006881714,\n",
              "  1.2133476734161377,\n",
              "  1.2136790752410889,\n",
              "  1.2137693166732788,\n",
              "  1.214325189590454,\n",
              "  1.2137300968170166,\n",
              "  1.2135348320007324,\n",
              "  1.213000774383545,\n",
              "  1.2124146223068237,\n",
              "  1.212309718132019,\n",
              "  1.212066411972046,\n",
              "  1.2130155563354492,\n",
              "  1.2119983434677124,\n",
              "  1.2114837169647217,\n",
              "  1.211453914642334,\n",
              "  1.2111058235168457,\n",
              "  1.2110875844955444,\n",
              "  1.2105443477630615,\n",
              "  1.210547924041748,\n",
              "  1.2100213766098022,\n",
              "  1.2092607021331787,\n",
              "  1.2093929052352905,\n",
              "  1.2094271183013916],\n",
              " 'dev_score_history': [0.3485,\n",
              "  0.3485,\n",
              "  0.3485,\n",
              "  0.3485,\n",
              "  0.3485,\n",
              "  0.3485,\n",
              "  0.3485,\n",
              "  0.3636,\n",
              "  0.3788,\n",
              "  0.3485,\n",
              "  0.3636,\n",
              "  0.3485,\n",
              "  0.3485,\n",
              "  0.3636,\n",
              "  0.3636,\n",
              "  0.3485,\n",
              "  0.3788,\n",
              "  0.3636,\n",
              "  0.3636,\n",
              "  0.3485,\n",
              "  0.3636,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788,\n",
              "  0.3788],\n",
              " 'test_score': 0.6316,\n",
              " 'train_loss_history': [0.997718095779419,\n",
              "  1.0112907604167336,\n",
              "  0.9974549004906103,\n",
              "  0.9918722980900815,\n",
              "  0.9859368330553958,\n",
              "  0.9788165500289515,\n",
              "  0.9729481402196383,\n",
              "  0.9695072581893519,\n",
              "  0.9622242733051902,\n",
              "  0.9486452466563174,\n",
              "  0.9438565906725431,\n",
              "  0.9397254052915072,\n",
              "  0.9292983506855211,\n",
              "  0.926346618878214,\n",
              "  0.9178761306561922,\n",
              "  0.9050457759907371,\n",
              "  0.8922893812781886,\n",
              "  0.90711426107507,\n",
              "  0.9028640326700712,\n",
              "  0.8919745966007835,\n",
              "  0.8921892141041002,\n",
              "  0.8928692654559487,\n",
              "  0.8852408810665733,\n",
              "  0.8892205859485426,\n",
              "  0.8775875066456041,\n",
              "  0.878792100831082,\n",
              "  0.878766517890127,\n",
              "  0.8794175982475281,\n",
              "  0.883398109360745,\n",
              "  0.8742271253937169,\n",
              "  0.8733941379346346,\n",
              "  0.8765415894357782,\n",
              "  0.8732976819339552,\n",
              "  0.8830185438457289,\n",
              "  0.8701842772333246,\n",
              "  0.8811087671079134,\n",
              "  0.8633188103374682,\n",
              "  0.8684630017531546,\n",
              "  0.8797629789302224,\n",
              "  0.8856815664391768,\n",
              "  0.8784825425398978,\n",
              "  0.8718954261980558,\n",
              "  0.87333660690408,\n",
              "  0.8692878108275565,\n",
              "  0.8792891188671714,\n",
              "  0.8808642688550448,\n",
              "  0.8721646321447272,\n",
              "  0.8820291977179678,\n",
              "  0.8704087075434233,\n",
              "  0.8604588665460285]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfEoE_G0BjAg"
      },
      "source": [
        "# ASSIGNMENT 3 (BONUS)\n",
        "\n",
        "+ TODO: Train the TextClassifier for the other 4 targets in the SemEval training data using the data files you have created in ASSIGNMENT 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_folder = '/content/drive/MyDrive/2022-ILTAPP/datasets/'\n",
        "corpus: Corpus = ClassificationCorpus(corpus_folder,\n",
        "                                      train_file='stance-semeval2016train.Legalization of Abortion.txt',\n",
        "                                      test_file='stance-semeval2016test.Legalization of Abortion.txt')\n",
        "\n",
        "\n",
        "# create the label dictionary\n",
        "label_dict = corpus.make_label_dictionary()\n",
        "\n",
        "# 3. make a list of word embeddings\n",
        "word_embeddings = [WordEmbeddings('en-crawl')]\n",
        "\n",
        "# 4. initialize document embedding by passing list of word embeddings                                                                                                                   \n",
        "                                                 \n",
        "embeddings_doc: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
        "                                             hidden_size=512,\n",
        "                                             reproject_words=True,\n",
        "                                             reproject_words_dimension=256,\n",
        "                                             rnn_type='LSTM')\n",
        "                                                                                                                                                                                         \n",
        "# 5. create the text classifier\n",
        "classifier = TextClassifier(embeddings_doc, label_dictionary=label_dict)\n",
        "# 6. initialize the text classifier trainer\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "\n",
        "#7. Training\n",
        "trainer.train('flair-abortion-model',\n",
        "              train_with_dev=False,\n",
        "              max_epochs=50)\n"
      ],
      "metadata": {
        "id": "zz6-qyN1J4of",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ddbf337-c4f9-4bf6-d33e-581f7db8e3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 16:01:53,192 Reading data from /content/drive/MyDrive/2022-ILTAPP/datasets\n",
            "2022-03-03 16:01:53,194 Train: /content/drive/MyDrive/2022-ILTAPP/datasets/stance-semeval2016train.Legalization of Abortion.txt\n",
            "2022-03-03 16:01:53,195 Dev: None\n",
            "2022-03-03 16:01:53,197 Test: /content/drive/MyDrive/2022-ILTAPP/datasets/stance-semeval2016test.Legalization of Abortion.txt\n",
            "2022-03-03 16:01:53,343 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 868/868 [00:01<00:00, 541.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 16:01:56,063 [b'AGAINST', b'NONE', b'FAVOR']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 16:02:04,228 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:02:04,232 Model: \"TextClassifier(\n",
            "  (document_embeddings): DocumentRNNEmbeddings(\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): WordEmbeddings('en-crawl')\n",
            "    )\n",
            "    (word_reprojection_map): Linear(in_features=300, out_features=256, bias=True)\n",
            "    (rnn): LSTM(256, 512, batch_first=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2022-03-03 16:02:04,235 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:02:04,238 Corpus: \"Corpus: 588 train + 65 dev + 280 test sentences\"\n",
            "2022-03-03 16:02:04,239 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:02:04,241 Parameters:\n",
            "2022-03-03 16:02:04,242  - learning_rate: \"0.1\"\n",
            "2022-03-03 16:02:04,244  - mini_batch_size: \"32\"\n",
            "2022-03-03 16:02:04,245  - patience: \"3\"\n",
            "2022-03-03 16:02:04,247  - anneal_factor: \"0.5\"\n",
            "2022-03-03 16:02:04,248  - max_epochs: \"50\"\n",
            "2022-03-03 16:02:04,249  - shuffle: \"True\"\n",
            "2022-03-03 16:02:04,251  - train_with_dev: \"False\"\n",
            "2022-03-03 16:02:04,253  - batch_growth_annealing: \"False\"\n",
            "2022-03-03 16:02:04,254 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:02:04,256 Model training base path: \"flair-abortion-model\"\n",
            "2022-03-03 16:02:04,258 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:02:04,260 Device: cuda:0\n",
            "2022-03-03 16:02:04,262 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:02:04,264 Embeddings storage mode: cpu\n",
            "2022-03-03 16:02:04,267 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-03 16:02:05,907 epoch 1 - iter 1/19 - loss 1.10753655 - samples/sec: 218.47 - lr: 0.100000\n",
            "2022-03-03 16:02:06,028 epoch 1 - iter 2/19 - loss 1.10090965 - samples/sec: 291.16 - lr: 0.100000\n",
            "2022-03-03 16:02:06,134 epoch 1 - iter 3/19 - loss 1.10087287 - samples/sec: 324.64 - lr: 0.100000\n",
            "2022-03-03 16:02:06,237 epoch 1 - iter 4/19 - loss 1.09900898 - samples/sec: 329.34 - lr: 0.100000\n",
            "2022-03-03 16:02:06,340 epoch 1 - iter 5/19 - loss 1.09278638 - samples/sec: 339.80 - lr: 0.100000\n",
            "2022-03-03 16:02:06,439 epoch 1 - iter 6/19 - loss 1.09600242 - samples/sec: 337.26 - lr: 0.100000\n",
            "2022-03-03 16:02:06,586 epoch 1 - iter 7/19 - loss 1.08173783 - samples/sec: 434.60 - lr: 0.100000\n",
            "2022-03-03 16:02:06,669 epoch 1 - iter 8/19 - loss 1.06824434 - samples/sec: 428.34 - lr: 0.100000\n",
            "2022-03-03 16:02:06,761 epoch 1 - iter 9/19 - loss 1.06320641 - samples/sec: 374.56 - lr: 0.100000\n",
            "2022-03-03 16:02:06,838 epoch 1 - iter 10/19 - loss 1.05782919 - samples/sec: 424.37 - lr: 0.100000\n",
            "2022-03-03 16:02:06,936 epoch 1 - iter 11/19 - loss 1.06120399 - samples/sec: 335.78 - lr: 0.100000\n",
            "2022-03-03 16:02:07,016 epoch 1 - iter 12/19 - loss 1.05560573 - samples/sec: 409.03 - lr: 0.100000\n",
            "2022-03-03 16:02:07,109 epoch 1 - iter 13/19 - loss 1.05055914 - samples/sec: 405.95 - lr: 0.100000\n",
            "2022-03-03 16:02:07,190 epoch 1 - iter 14/19 - loss 1.04988893 - samples/sec: 405.17 - lr: 0.100000\n",
            "2022-03-03 16:02:07,270 epoch 1 - iter 15/19 - loss 1.05144104 - samples/sec: 405.62 - lr: 0.100000\n",
            "2022-03-03 16:02:07,351 epoch 1 - iter 16/19 - loss 1.03320418 - samples/sec: 433.80 - lr: 0.100000\n",
            "2022-03-03 16:02:07,436 epoch 1 - iter 17/19 - loss 1.00809825 - samples/sec: 446.75 - lr: 0.100000\n",
            "2022-03-03 16:02:07,516 epoch 1 - iter 18/19 - loss 1.01978441 - samples/sec: 409.74 - lr: 0.100000\n",
            "2022-03-03 16:02:07,560 epoch 1 - iter 19/19 - loss 1.02131427 - samples/sec: 803.87 - lr: 0.100000\n",
            "2022-03-03 16:02:08,056 ----------------------------------------------------------------------------------------------------\n",
            "2022-03-03 16:02:08,058 EPOCH 1 done: loss 1.0213 - lr 0.1000000\n",
            "2022-03-03 16:02:09,657 DEV : loss 1.2520016431808472 - score 0.4769\n",
            "2022-03-03 16:02:09,734 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ TODO: Write in a table in this notebook the results obtained for the 5 targets. HINT: check the table in Lab2.\n"
      ],
      "metadata": {
        "id": "X7GOhyl0az_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8ovDcqlla1PW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "1oBgB9WKcowC",
        "outputId": "5b1f59a1-a364-4cce-949d-a8d01cfe8317"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b9e8c65a136b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fscore'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    }
  ]
}